\documentclass[twocolumn,linenumbers]{aastex631}
\usepackage{physics}
\usepackage{gensymb}
\usepackage{CJK}

% Operators
\DeclareMathOperator*{\argmax}{arg\,max}
\DeclareMathOperator*{\argmin}{arg\,min}
\DeclareMathOperator*{\diag}{diag}
\newcommand{\exptval}[1]{\mathbb{E}\left[ #1 \right]}
\newcommand{\Var}[1]{\text{Var}\qty[#1]}
\newcommand{\Cov}[1]{\text{Cov}\qty[#1]}

% variables
\newcommand{\bb}[1]{\mathbb{#1}}
\newcommand{\vbd}{\vb{d}}
\newcommand{\vbm}{\vb{m}}
\newcommand{\vbep}{\vb{\varepsilon}}
\newcommand{\vbn}{\vb{n}}
\newcommand{\vbb}{\vb{b}}
\newcommand{\inv}[1]{#1^{-1}}
\newcommand{\noisevar}{\Sigma_{\vbn}}
\newcommand{\hatm}{\vb{\hat{m}}}
\newcommand{\Pdagger}{P^{\dagger}}
\newcommand{\Nbar}{\bar{N}}
\newcommand{\PPinv}[1]{\inv{\qty(\Pdagger #1 P)}}
\newcommand{\Neta}{N_{\eta}}

\newcommand{\kmh}[1]{\textcolor{red}{KMH: #1}}


%\submitjournal{ApJ}
\graphicspath{{./}{figures/}}

\begin{document}

%\title{Perturbative Approach to Solve the Map-Making Equation}
%\title{Cosmic Microwave Background map-making solutions with a cooled or perturbative approach}
\title{Cosmic Microwave Background map-making solutions improve with cooling}


  \author{Bai-Qiang Qiang (\kmh{want chinese characters?})} % KMH: I don't know how to do it...
\affiliation{Department of Physics, Florida State University, Tallahassee, Florida 32306}

\author[0000-0001-7109-0099]{Kevin M. Huffenberger}
\affiliation{Department of Physics, Florida State University, Tallahassee, Florida 32306}

 
\begin{abstract}
%  \kmh{I think we maybe need a new title?  Perturbation usually means a small change, whereas this is a big change that we apply gradually.  Although I guess we are building up the final solution but repeated small perturbations. Word ideas: ``Linear system, map-making, Cosmic Microwave Background, annealing''.  It's the same as cooling, but cooling doesn't make sense.}

  In the context of Cosmic Microwave Background data analysis, we study the solution to the equation that transforms scanning data into a map.  As originally suggested in ``messenger'' methods for solving linear systems, we split the  noise covariance into uniform and non-uniform parts and adjusting their relative weight during the iterative solution.  This ``cooling'' or perturbative approach is particularly effective when there is significant low-frequency noise in the timestream.  A conjugate gradient algorithm applied to this modified system converges faster and to a higher fidelity solution than the standard conjugate gradient approach, for the same computational cost per iteration.  We conclude that cooling is helpful separate from its appearance in the messenger methods.
%We present a method to solve map-making equation by reinterpret messenger field method and using conjugate gradient algorithm.
%We separating noise covariance matrix $N$ into two parts, white noise part and the remaining noise part as perturbation, with perturbation parameter $\eta$.
%
We give an analytical expression for the parameter that controls how gradually should change during the course of the solution. 

\end{abstract}

\keywords{Computational methods --- Cosmic microwave background radiation --- Astronomy data reduction}

\section{Introduction} \label{sec:intro}

%The cosmic microwave background is an electromagnetic radiation coming from early
%stage of our Universe. Based on the hot Big Bang model, before the recombination
%epoch, the photons were tightly coupled with free electrons and protons via 
%Thomson scattering. As the universe expanded and cooled down, free electrons and
%protons combined into neutral hydrogen atom.  This process is called 
%recombination in cosmology. Shortly after this epoch, the photons could
%propagate freely and would not be scattered by charged particles. Today, we receive
%the photons produced at that time and called them the cosmic microwave background
%radiation. Studying these photons coming from early universe can help us
%to constrain theoretical model and cosmological parameters
%\cite{10.1093/ptep/ptaa104}.
%The next generation CMB observations will have much higher resolution and 
%generate more data. So we need an efficient way to process the data.
%One step of the the processing is map making, which gives an estimated map based on the raw observation data.

%map-making

In observations of the Cosmic Microwave Background (CMB), map-making is an intermediate step between the collection of raw scanning data and the scientific analyses, such as the estimation of power spectra and cosmological parameters.
Next generation CMB observations will generate much more data than those today, and so
it is worth exploring efficient ways to process the data, even though, on paper, the map-making problem has long been solved.

The time-ordered scanning data is summarized by
\begin{equation}
\vbd = P\vbm + \vbn \label{map making model}
\end{equation}
where $\vbd$, $\vbm$, and $\vbn$ are the vectors of time-ordered data (TOD), the CMB sky-map signal, and measurement noise, and $P$ is the sparse matrix that encodes the telescope's pointing.  Of several mapmaking methods \citep{1997ApJ...480L..87T}, one of the most common is the method introduced for the Cosmic Background Explorer \cite[COBE,][]{1992ASIC..359..391J}.  This optimal, linear solution is 
\begin{align}
\qty(\Pdagger \inv{N}  P) \hatm = \Pdagger \inv{N} \vbd \label{map making eq}
\end{align}
where  $\hatm$ provides the generalized least squares minimization of the $\chi^2$ statistic
\begin{align}
\chi^2(\vbm) & \equiv (\vbd - P\vbm)^{\dagger} N^{-1} (\vbd - P \vbm).
\label{chi2 formula}
\end{align}
Here we assume that the noise has zero mean $\ev{\vbn} = \vb{0}$,
and noise covariance matrix is $N = \ev{\vbn \vbn^{\dagger}}$. Thus  mapmaking is a standard linear regression problem.
In case the noise is Gaussian, the COBE solution is also the maximum likelihood solution.

With current computation power, we cannot solve for $\hatm$
by calculating $\PPinv{\inv{N}} \Pdagger \inv{N} \vbd$ directly, since the $(P^\dag \inv{N}P)$ matrix is too large to invert.  The noise covariance matrix $N$ is often sparse in frequency domain and the pointing matrix $P$ is sparse in the time-by-pixel domain, and their product is dense.   In experiments currently under design, there may be $\sim 10^{16}$ time samples and $\sim 10^{9}$ pixels, so these matrix inversions are intractable.
%It is impossible to do these matrix multiplication directly and then take
%inverse.
We can use iterative methods, such as conjugate gradient descent, to avoid the matrix inversions, and execute each matrix multiplication in a basis where the matrix is sparse, using a fast Fourier transform to go between the frequency and time domain.

As an alternative to conjugate gradient descent, \citet{Huffenberger_2018} showed that the ``messenger'' iterative method could be adapted to solve the linear mapmaking system, based on the  approach from \cite{2013A&A...549A.111E} to solve the linear Wiener filter.  This technique splits the noise covariance into a uniform part and the remainder, and introduces an additional vector that represent the signal plus uniform noise.  This messenger field acts as an intermediary between the signal and the data and has a covariance that is conveniently sparse in every basis.  \cite{2013A&A...549A.111E} also introduced a cooling scheme that takes  advantage of the split covariance: over the course of the iterative solution, we adjust the relative weight of the two parts.  Starting with the uniform covariance, the modified linear system gradually transforms to the final system, under the control of a cooling parameter.  In numerical experiments, \citet{Huffenberger_2018} found that a map produced by the cooled messenger method converged significantly faster than for standard conjugate gradient methods, and to higher fidelity, especially on large scales.  

\citet{2018A&A...620A..59P} showed that the the messenger field approach is equivalent to a fixed point iteration scheme, and studied its convergence properties in detail.  Furthermore, they showed that the split covariance and the modified system that incorporates the cooling can be solved by other means, including a conjugate gradient technique, which should generally show better convergence properties than the fixed-point scheme. However in numerical tests, \citet{2018A&A...620A..59P} did not find benefits to the cooling modification of the mapmaking system, in contrast to findings of \citet{Huffenberger_2018}.

In this paper, we show that the difference arose because the numerical tests in \citet{2018A&A...620A..59P} used much less low-frequency (or $1/f$) noise than \citet{Huffenberger_2018}, and show that the cooling technique improves mapmaking performance especially when the low frequency noise is large.  This performance boost depends on a proper choice for the pace of cooling.  \citet{2017MNRAS.468.1782K} showed that for Wiener filter the cooling parameter should be chosen as a geometric series.  In this work, we give an alternative interpretation of the parameterizing process and show that for map-making the optimal choice (unsurprisingly) is also a geometric series.

%The time-ordered data we collected is given by map signal $P\vbm$ plus noise 
%$\vbn$.
%The pointing matrix $P$ acting on the map gives the signal of the map at some specific 
%position of sky where telescope is pointing at.

%As we can see the map making model Eq.~(\ref{map making model}) mathematically 
%is a standard linear regression problem,
%with \textit{design matrix} being pointing matrix $P$, and \textit{regression
%coefficients} are $\vbm$.
%For COBE method, we estimate linear regression coefficients $\vbm$ with
%\textit{generalized least square} (GLS) technique, since the noise $\vbn$ is 
%\textit{heteroscedastic}.
%: the variances $N$ are different for various frequencies.
%Usually detectors have a $1/f$ noise pattern
%\cite{1997PhRvD..56.4514T}.
%The \textit{generalized least square} (GLS) will provide better estimation 
%than \textit{ordinary least square} (OLS) method, because the data is
%heteroscedastic so we would like to focusing on fitting the data with lower 
%noise.
%then the estimated map $\hatm$ minimize
%The GLS minimize

%and the estimated map $\hatm$ is given by
%\begin{equation}
%\hatm = \argmin_{\vbm}  \chi^2(\vbm) = \PPinv{N} \Pdagger \inv{N} \vbd 
%\end{equation}
%Or rewrite it as


In Section \ref{sec:methods} we describe our methods for treating the mapmaking equation and our numerical experiments.  In Section \ref{sec:results} we present our results. In Section \ref{sec:discussion} we interpret the mapmaking approach and its computational cost.  In Section \ref{sec:conclusions} we conclude.  In appendices we derive the prescription for our cooling schedule.


\section{Methods}\label{sec:methods}

\subsection{Parameterized Conjugate Gradient Method}
The messenger field approach introduced an extra cooling parameter $\lambda$ to the
map-making equation, and solved the linear system with the alternative covariance $N(\lambda) =  \lambda \tau I + \Nbar $.  The parameter $\tau$ represents the uniform level of (white) noise in the covariance, $\Nbar$ is the balance of the noise covariance, and the parameterized covariance equals the original covariance when the cooling parameter $\lambda = 1$.  In this work we find it more convenient to work with the inverse cooling parameter $\eta = \lambda^{-1}$ and define the covariance as
\begin{equation}
  N(\eta) = \tau I +  \eta \Nbar 
\end{equation}
which leads to the same system of mapmaking equations.  (This is because $N(\eta) = \lambda^{-1} N(\lambda)$ and the mapmaking equation (Eq.~\ref{map making eq eta}) is insensitive to to scalar multiple of the covariance since is appears on both sides.)
Since the non-white part $\bar N$ is the troublesome portion of the covariance, and 
we can think of the $\eta$ parameter as turning it on slowly, adding a perturbation to the solution achieved at a particular stage, building ultimately upon the initial uniform covariance model.


\citet{2018A&A...620A..59P} showed that the conjugate gradient method can be easily applied to the parameterized map-making equation by iterating on
\begin{align}
\Pdagger \inv{N(\eta)}  P\ \hatm = \Pdagger \inv{N(\eta)} \vbd
\label{map making eq eta}
\end{align}
as we adjust the parameter.  In our numerical experiments, we confirm that the conjugate gradient approach is converging faster than the fixed point iterations suggested by the messenger mapmaking method in \citet{Huffenberger_2018}.  For simplicity we fix the preconditioner to $M= \Pdagger P$ for all of calculations.

%Based on previous analysis, we know that what messenger field method really
%does is parameterizing the map-making equation.
%Here to avoid confusion, we introduce another parameter $\eta$, such that the 
%Here we now show that we can improve performance in some cases by using a parameterized version of the map making equation Eq.~(\ref{map making eq}).
%The idea is that map-making equation Eq.~(\ref{map making eq}) is hard to solve
%due to noise covariance matrix is sandwiched between $\Pdagger P$.

When $\eta = 0$, the noise covariance matrix $N(0)$ is proportional to identity matrix $I$, and solution is given by simple binned map
$\vbm_0 = \inv{\qty(\Pdagger P)} \Pdagger \vbd$,
which can be solved directly.
The non-uniform part $\bar N$ is the troublesome portion of the covariance, and 
we can think of the $\eta$ parameter as turning it on slowly, adding a perturbation to the solution achieved at a particular stage, building ultimately upon the initial uniform covariance model.


From the starting point, the cooling scheme requires the inverse cooling parameter $\eta$ increase as $0 = \eta_0 \leq \eta_1 \leq \cdots \leq \eta_{\rm final} = 1$, at which point we arrive at the desired mapmaking equation.  We may iterate more than once at each intermediate $\eta_i$: we solve   with conjugate gradient iterations
\begin{align}\qty(\Pdagger \inv{N(\eta_i)} P)\, \hatm(\eta_i) = \Pdagger \inv{N(\eta_i)} \vbd, \end{align}
using the result from previous calculation $\hatm(\eta_{i-1})$ as the initial value,
and move to next parameter $\eta_{i+1}$ when the residual 
$\qty(\Pdagger \inv{N(\eta_i)} P)\, 
\hatm(\eta_i) - \Pdagger \inv{N(\eta_i)} \vbd \simeq \vb{0}$.
\textcolor{green}{
During our calculation we move to next $\eta$ parameter when the norm of residual $\norm{\vb{r}}$
is smaller than one tenth of standard deviation of each pixel,
under the assumption that the noise is purely white with power spectrum $P(f) =\sigma^2$, which is
the minimum value of our model Eq.~(\ref{noise power spectrum}).
}
%The initial guess is $\hatm(\eta_0) = \vbm_0 = \PPinv{} \Pdagger \vbd$.
\kmh{I think you have to be specific about the residual threshold.}



\subsection{Choice of inverse cooling parameters $\eta$}
The next question is how to choose these monotonically increasing parameters
$\eta$. 
If we choose them inappropriately,  the solution converges
slowly, because we waste effort converging on the wrong system.
We also want to determine $\eta_1, \cdots, \eta_{n-1}$ before starting conjugate
gradient iterations.  The time ordered data $\vbd$ is very large,
and we do not want to keep it in the system memory during calculation.  If we determine $\eta_1, \cdots, \eta_{n-1}$ before the iterations, 
then we can precompute the right-hand side $\Pdagger \inv{N(\eta)} \vbd$ for each $\eta_i$ and keep these map-sized objects in memory, instead of the entire time-ordered data.

In the appendix, we show that a generic good choice for the $\eta$ parameters are the geometric series
\begin{align}
\eta_i =\min \qty\bigg{ \qty(2^i -1)\frac{\tau}{\max(\Nbar_f)},\; 1 },
\label{etai rule}
\end{align}
where $\bar N_f$ is the frequency representation of the non-uniform part of the covariance.  This is the main result.  It tells us not only how to choose parameters $\eta_i$,
but also when we should stop the perturbation, and set $\eta = 1$.
For example, if noise covariance matrix $N$ is almost white noise,
then $\Nbar = N - \tau I \approx 0$,
and we would have ${\tau}/{\max(\Nbar_f)} \gg 1$.
This tell us that we don't need to use parameterized method at all, 
because $\eta_0=0$ and $\eta_1= \eta_2 = \cdots= 1$.
This corresponds to the standard conjugate gradient method with simple binned 
map as the initial guess (as recommended by \citealt{2018A&A...620A..59P}).


\subsection{Computational Cost}
To properly compare the performance cost of this method with respect to vanilla
conjugate gradient method with simple preconditioner,
we need to compare their computational cost at each iteration.
The right hand side of parameterized map-making equation
Eq.~(\ref{map making eq eta})
%\begin{align}
%\qty(\Pdagger \inv{N(\eta)} P)\, \hatm(\eta) 
%= \Pdagger \inv{N(\eta)} \vbd \tag{\ref{map making para}}
%\end{align}
could be computed before iterations,
so it won't introduce extra computational cost.
The most demanding part of conjugate gradient method is calculating
$\Pdagger \inv{N} P \hatm$, because it contains a Fourier transform of
$P\hatm$ from time domain to frequency domain and an inverse Fourier transform
of $\inv{N} P \hatm$ from frequency domain back to time domain,
which is order $\mathcal{O}(n\log n)$ with $n$ being the length of time ordered
data.
If we change $\inv{N}$ to $\inv{N(\eta)}$, it won't add extra cost,
since both matrices are diagonal in frequency domain.
Therefore the computational cost it the same for one step.

However our previous analysis is based on
$\chi^2(\hatm(\eta_i), \eta_i)$ which is evaluated at 
$\hatm(\eta_i)$ the estimated map at $\eta_i$.
So We should update $\eta_i$ to $\eta_{i+1}$ when $\vbm \approx \hatm(\eta_i)$. 
How do we know this condition is satisfied?
Since for each new $\eta_i$ value, we are solving a new set of linear
equations $A(\eta_i) \hatm = \vbb(\eta_i)$ with
$A(\eta_i) = \Pdagger \inv{N(\eta_i)} P$ and 
$\vbb(\eta_i) = \Pdagger \inv{N(\eta_i)} \vbd$,
and we could stop calculation and moving to next value $\eta_{i+1}$ when the 
norm of residual 
$\qty||\vb{r}(\eta_i)|| = \qty||\vbb(\eta_i) - A(\eta_i) \vbm||$
smaller than some small value.
%Since when doing conjugate gradient algorithm we calculate $\vb{r}$ and stop
%the iteration when $\qty||\vb{r}||$ is small enough, now after introducing
%parameter $\eta$, we move to next parameter $\eta_{i+1}$ when 
%$\qty||\vb{r}(\eta_i)||$ is small enough.
Calculate $\qty||\vb{r}(\eta_i)||$ is part of conjugate gradient algorithm,
so this won't add extra cost compare to vanilla conjugate gradient method.
Therefore, overall introducing $\eta$ won't have extra computational cost.



\subsection{Numerical Simulations}
\begin{figure}[]
\includegraphics[width=0.8\linewidth]{P_f.pdf}
\centering
\caption{The noise power spectrum based on Eq.~(\ref{noise power spectrum}) with 
    $\sigma^2 = 10$ $\mu$K$^2$ and $\alpha = 3$.
    Two knee frequencies $f_\text{knee}=10$ Hz (solid lines) 
    and $f_\text{knee}=0.1$ Hz (dashed lines).
    For each knee frequency, we have $f_\text{apo}=0$ Hz, $0.1f_\text{knee}$ and
    $0.01f_\text{knee}$.
    The vertical line shows our scanning frequency. \kmh{try to put the legend below the figure so the plot is not so small.}
}
\label{power spectrum}
\end{figure}

To compare these algorithms, we need to do some simple simulation of scanning
processes, and generate time ordered data from a random sky signal.\footnote{
The source code and other information are available at \url{https://github.com/Bai-Qiang/map_making_perturbative_approach}
}
Our sky is a small rectangular area, with two orthogonal directions $x$ and
$y$, both with range from $-1\degree$ to $+1\degree$.
The signal has stokes parameters $(I,Q,U)$ for intensity and linear polarization.
%We model the overall electromagnetic signal is created by some normal
%distributed sources in the sky, with intensity $I_i (x,y)
%= A_i \exp\qty(-\frac{1}{2} \frac{(x-x_{i})^2 + (y-y_{i})^2}{\sigma_i^2})$,
%for each source centered at $(x_i,y_i)$.
%In our simulation, $A_i \sim \text{Unif} (-100, 100)$,
%$\sigma_i \sim \text{Unif}(0.05\degree, 0.2\degree)$ 
%and the center of each source
%$x_i, y_i \sim \text{Unif}(-1\degree, +1\degree)$.
%Every source has its degree of polarization $p_i \sim \text{Unif}(0,1)$ and 
%polarization angle $\psi_i \sim \text{Unif}(0,\pi)$.
%Here we ignored angle $\chi_i$, because our detectors won't be sensitive to
%circular polarization.
%Finally, the stokes parameters over sky is given by
%$S_0(x,y) = \sum_i I_i(x,y)$, $S_1(x,y) = \sum_i I_i(x,y) p_i \cos(2\psi_i)$,
%$S_2(x,y) = \sum_i I_i(x,y) p_i \sin(2\psi_i)$.
%Again, we ignored $S_3$, because it describes circular polarization.

For the scanning process, our mock telescope contains nine detectors,
each with different sensitivity to polarization $Q$ and $U$.
It scans the sky with a raster scanning pattern and scanning frequency
$f_{\text{scan}} = 0.1$ Hz and sampling frequency $f_{\text{sample}} = 100$ Hz.
The telescope scans the sky horizontally and then vertically,
and then digitizes the position $(x, y)$ into $512\times 512$ pixels.
This gives noiseless signal $\vb{s} = P\vb{m}$.

We model the noise power spectrum with
\begin{align}
P(f) = \sigma^2 \qty(1+ \frac{f_{\text{knee}}^{\alpha}+f_{\text{apo}}^{\alpha}}
    {f^{\alpha}+f_{\text{apo}}^{\alpha}}) \label{noise power spectrum}
\end{align}
which is white at high frequencies, a power law below the knee frequency, and gives us the option to flatten the low frequency noise below an apodization frequency \citep[like in][]{2018A&A...620A..59P}.
Note that as $f_{\text{apo}} \rightarrow 0 $,
$P(f) \rightarrow \sigma^2\qty(1 + (f/f_{\text{knee}})^{-\alpha} )$, 
and it becomes a $1/f$ noise model.

\citet{2013ApJ...762...10D} measured the slopes of the atmospheric noise in the Atacama under different water vapor conditions, finding $\alpha = 2.7$ to $2.9$.
Here we fixed $\sigma^2 = 10$ $\mu$K$^2$, $\alpha=3$, and $f_{\text{knee}} = 10$ Hz,
and change $f_{\text{apo}}$ to compare the performance under different noise
models.

The noise covariance matrix 
\begin{equation}
N_{ff'} = P(f) \frac{\delta_{ff'}}{\Delta_f}
\label{noise covariance matrix}
\end{equation}
is a diagonal matrix in frequency space, where $\Delta_f$ is equal to reciprocal
of total scanning time $T \approx 1.05\times 10^{4}$ seconds.
In our calculations we choose different combination of $f_\text{knee}$ and $f_\text{apo}$,
some of the power spectrum are shown in Figure~\ref{power spectrum}.

Finally, we get the simulated time ordered data $\vb{d} = \vb{s} + \vb{n}$ by
adding up signal and noise.



\section{Results} \label{sec:results}

\begin{figure*}[]
\centering
\includegraphics[width=0.8\textwidth]{pink_noise_chi2.pdf}
\caption{
%These three figures show the 
%${(\chi^2(\vbm) - \chi^2_\text{min})}/{(\chi^2_\text{ini} - \chi^2_\text{min})}$
%changes for each iteration under different noise covariance matrix with
%fixed $f_\text{apo}=0$ and $f_\text{knee}$ being $0.1$, $0.5$, and $1.0$.
    Here we show the $\chi^2(\vbm)$ with respect to number of iterations.
    The vertical axis is rescaled  such that all curves start from 1.
    The mapmaking equation (Eq.~\ref{map making eq}) minimize the $\chi^2(\vbm)$, so
    the curve which goes down fast and get close to zero at the end is the preferred method.
    In this figure we are comparing traditional conjugate gradient method labeled as \textit{CG} (blue line)
    with parameterized conjugate gradient labeled as \textit{CG with $\eta$} (orange line)
    under different $1/f$ noise model (fixed $f_\text{apo}=0$ Hz but different $f_\text{knee}$).
    As we can see here when $f_\text{knee} \gtrsim 10\,f_\text{scan} = 1$ Hz, there are significant amount of
    low frequency noise and the parameterized conjugate gradient method starts showing advantages.
}
\label{1/f noise chi2}
\end{figure*}

\begin{figure*}[]
\centering
\includegraphics[width=0.8\textwidth]{apodized_noise_chi2.pdf}
\caption{
    The vertical and horizontal axes are the same as in Figure~(\ref{1/f noise chi2}),
    and also compare traditional conjugate gradient method labeled as \textit{CG} (blue line) 
    with parameterized conjugate gradient method labeled as \textit{CG with $\eta$} (orange line).
    But here we fix the knee frequency $f_\text{knee} = 10$ Hz, and change apodized frequency $f_\text{apo}$.
    When $f_\text{apo}$ is much smaller than $f_\text{knee}$, there are more low frequency noise and
    parameterized conjugate gradient method is better than traditional ones.
}
\label{apo noise chi2}
\end{figure*}



We first compare the vanilla conjugate gradient method with
simple preconditioner $\Pdagger P$ versus conjugate gradient with our perturbed linear system.
%The results are showed in Figure~(\ref{1/f noise chi2}) and Figure~(\ref{apo noise chi2})
%for different kinds of noise power spectra.
Figure~(\ref{1/f noise chi2}) shows the $\chi^2(\vbm)$ results for 1/f noise model ($f_\text{apo}=0$)
with different knee frequencies.
Note that $\chi^2$ in all figures are calculated based on
Eq.~(\ref{chi2 formula})
not $\chi^2(\vbm, \eta)$ in Eq.~(\ref{chi2 eta formula}).
And the $\chi^2_{\text{min}}$ is calculated from perturbative conjugate gradient
method with 100 $\eta$ values, and it stops when the norm of residual 
$\norm{\vb{r}} = \norm{\Pdagger \inv{N}\vbd - (\Pdagger\inv{N}P) \vbm}$
per pixel is smaller than $10^{-10}$, or after 1000 iterations.
%\kmh{Whenever we show a plot, we need to explain what the reader is to take away from it.  Don't just jump to the next plot.}
From Figure~(\ref{1/f noise chi2}) we can see (also combine the left graph in Figure~(\ref{apo noise chi2})),
for $1/f$ noise model, when $f_\text{knee} \gtrsim 10 f_\text{scan}$
the parameterized method starts showing advantage over vanilla conjugate gradient method.

In Figure~(\ref{apo noise chi2}) we fixed $f_\text{knee}=10$ Hz, and change $f_\text{apo}$.
When $f_\text{apo}$ is much smaller than $f_\text{knee}$ the parameterized conjugate gradient method would
performs better.
As we increase $f_\text{apo}$ while fix $f_\text{knee}$, eventually these two methods perform similar.

If we look at the power spectrum in Figure~(\ref{power spectrum}),
when $f_\text{knee}$ is small or $f_\text{apo}$ is large there are not many
large scale low frequency noise.
So we conclude that by introducing $\eta$ parameter could improve perform when there are large low noise
contribution.


We also tried different $\alpha$ values. For $\alpha=2$, the conclusion is the
same as $\alpha=3$. When $\alpha=1$, there are not many low frequency noise, 
the vanilla conjugate gradient is preferred, except some cases with very large
knee frequency like $f_\text{knee} = 100$ Hz and $f_\text{apo}=0$ would favor
parameterized method.
In \citealt{2018A&A...620A..59P}, the $\alpha = 1$ and  the noise power spectrum is apodized at $0.1f_\text{knee}$,
which corresponds to $f_\text{apo} \approx 0.1 f_\text{knee}$,
and their knee frequency is the same as scanning frequency, so $f_\text{knee}=f_\text{scan}=0.1$ 
in our cases.
In their case there are not many low frequency noise,
and we confirm that vanilla conjugate gradient method would converge faster.



\section{Discussion} \label{sec:discussion}

\subsection{Intuitive Interpretation of $\eta$}\label{intuitive interp}

\kmh{most of this is pretty similar to discussion in Huffenberger and Naess.  The last paragraph is new.}

In this section, let me introduce another way to understand the role of $\eta$.
Our ultimate goal is to find $\hatm(\eta=1)$ which minimizes 
$\chi^2(\vbm) = (\vbd - P\vbm)^{\dagger} \inv{N} (\vbd - P\vbm)$.
Since $N$ is diagonal in frequency space,
$\chi^2$ could be written as a sum of all frequency mode 
$\qty|(\vbd-P\vbm)_f|^2$ with weight $\inv{N}_f$, such as
$\chi^2(\vbm) = \sum_f \qty|(\vbd-P\vbm)_f|^2 \inv{N}_f$.
$\inv{N}_f$ is large when there is little noise at that frequency,
and vice versa.
Which means $\chi^2(\vbm)$ would favor the low noise frequency mode over high 
noise ones.
In other words the optimal map $\hatm$ focusing on minimize the error
$\vb{r} \equiv \vbd - P\vbm$ in the low-noise part.

After introducing $\eta$, we minimize
$\chi^2(\vbm,\eta)=(\vbd-P\vbm)^{\dagger} N_{\eta}^{-1} (\vbd - P\vbm)$.
%for each $\eta$ value as it increase from $0$ to $1$.
For $\eta=0$, $N^{-1}_{\eta=0} \propto I$ and the estimated map $\hatm(\eta=0)$
does not prioritize any frequency mode.
As we slowly increase $\eta$, we decrease the weight for the frequency modes
which have large noise, and focusing minimizing error for low noise part.
If we start with $\eta_1=1$ directly, which corresponds to the vanilla conjugate
gradient method, then the entire conjugate gradient solver
will focus most on minimizing the low noise part, such that $\chi^2$ would
converge very fast at low noise region, but slowly on high noise part.
\textcolor{green}{
Since it focus on low noise part only, it may be stuck at some local minimum point.
To get to the global minimum, it need to adjust the low noise part, that would
be difficult if it's stuck at an local minimum.
}
However by introducing $\eta$ parameter, we let the solver first treat every
frequency equally.
Then as $\eta$ slowly increases, it gradually shifts focus from the highest noise to the lowest noise
part.  \kmh{I feel what this is missing is why the high-noise modes get stuck though.}


If we write the difference between final and initial $\chi^2$ value as
$\chi^2(\hatm(1),1) - \chi^2(\hatm(0),0) = \int_0^1 \dd{\eta}
\dv{\eta} \chi^2(\hatm(\eta),\eta)$,
and use Eq.~(\ref{d chi2}).
%\begin{align}
%\dv{\eta} \chi^2(\hatm(\eta), \eta) 
%&= - \qty(\vbd - P\hatm(\eta))^{\dagger} \inv{\Neta} \Nbar \inv{\Neta} 
%    (\vbd - P\hatm(\eta)) \tag{\ref{d chi2}}
%\end{align}
We note that when $\eta$ is very small, 
the $\dv{\eta}\chi^2(\hatm(\eta),\eta)$ would have relatively large
contribution from medium to large noise region, comparing to large $\eta$.
So introducing $\eta$ might improve the convergence of $\chi^2$ at these
regions, because the vanilla conjugate gradient method only focuses on the low noise
part and it may have difficulty at these regions.



\subsection{Other $\eta$ Choices}

\begin{figure*}[htb!]
\centering
\includegraphics[width=0.8\textwidth]{chi2_neta.pdf}
\caption{
    The horizontal and vertical axes are the same as in the previous figure.
    The blue line and the orange line are traditional conjugate gradient method
    and parameterized conjugate gradient method.
    For three extra lines, we fix the number of $\eta$ parameter $n_{\eta}$ manually.
    The $\eta$ series are determined by \texttt{numpy.logspace(start=$\ln(\eta_1)$, stop=0, num=$n_{\eta}$, base=$e$)},
    instead of Eq.~(\ref{etai rule}).
    The first graph shows in some cases the $\eta$ series given by Eq.~(\ref{etai rule}) is ideal,
    but the second and third graph show that Eq.~(\ref{etai rule}) may ends up too many $\eta$
    which yields slower convergence.
}
\label{chi2 neta}
\end{figure*}

Now let us compare the performance difference between choosing $\eta$
parameters based on Eq.~(\ref{etai rule})
and fixing number of $\eta$ parameters $n_{\eta}$ manually.
We choose the $\eta_i$ values using function
\texttt{numpy.logspace(start=$\ln(\eta_1)$, stop=0, num=$n_{\eta}$, base=$e$)}.
The results are showed in Figure~(\ref{chi2 neta}).

In some cases the $\eta$ series determined by Eq.~(\ref{etai rule}) is ideal
(the first graph in Figure~(\ref{chi2 neta})), in other cases Eq.~(\ref{etai rule})
gives too many $\eta$ values such that it is not optimal (the second and third
graph in Figure~(\ref{chi2 neta})).



\subsection{Future Prospects}

\begin{figure*}[htb!]
\centering
\includegraphics[width=0.8\textwidth]{chi2_exact_eta.pdf}
\caption{
    The blue line and orange line is the same as in Figure~(\ref{chi2 neta}) for reference.
    The extra green line shows the result when $\delta\eta_m$ is determined from 
    Eq.~(\ref{exact eta}) not from Eq.~(\ref{etai rule}).
    This shows that if we could update based on exact expression Eq.~(\ref{exact eta}),
    it could converge even faster.
    Especially in the third graph it would overcome the shortcomings of parameterized conjugate gradient method.
}
\label{chi2 exact eta}
\end{figure*}


In Appendix~\ref{appendix:eta calculation}, we determine $\delta\eta_m$ value based on the upper bound of 
$-\delta\chi^2(\hatm(\eta_m), \eta_m)/\chi^2(\hatm(\eta_m), \eta_m)$, and choose $\delta\eta_m$ such that 
the upper bound is equal to $1$.
The reason we use this upper bound instead of using
\begin{align}
\delta\eta_m=-\chi^2(\hatm(\eta_m), \eta_m)/\dv{\eta}\chi^2(\hatm(\eta_m),\eta_m)
\label{exact eta}
\end{align}
directly, is that we don't want to keep the time ordered data $\vbd$ in system memory.
In Figure~(\ref{chi2 exact eta}) we can see if we use Eq.~(\ref{exact eta}) for each
$\delta\eta_m$, indeed it can improve performance. 
Especially for the third graph where the power spectrum does not have lots of
low frequency noise by using Eq.~(\ref{chi2 exact eta}) the result is close to
vanilla conjugate gradient method, which overcomes the shortcomings of parameterized
conjugate gradient method.
Therefore, to further improve this method, we need to find more accurate expression for Eq.~(\ref{dchi2 chi2}).



%As you may have noticed in the second and third Figure(\ref{f scan 0.1}),
%the perturbation parameter based on
%Eq.~(\ref{etai rule}) is more than needed, especially for $1/f$ noise case.
%For the case $\kappa=10^{12}$, we notice that based on Eq.~(\ref{etai rule})
%it gives us $n_{\eta}\approx40$, however from $\chi^2$
%result in the last Figure(\ref{f scan 0.1}) 
%$n_{\eta}\approx30$ or even $n_{\eta} \approx 15$ is good enough.
%Also, for the nearly-white-noise case, we could certainly choose $n_{\eta}=1$
%such that $\eta_1=1$ which corresponds to vanilla conjugate gradient method,
%based on $\chi^2$ result in first Figure(\ref{f scan 0.1}).
%However Eq.~(\ref{etai rule}) gives us $n_{\eta} \approx 6$,
%even though it does not make the final $\chi^2$ result much different at the
%end.
%
%Is it possible to further improve the analysis, such that it produces
%smaller $n_{\eta}$?
%Let's examine how we get $\eta_i$ series.
%Remember that we determine $\delta\eta$ value based on the upper bound of 
%$-\delta\chi^2(\hatm(\eta), \eta)/\chi^2(\hatm(\eta), \eta)$, in
%Eq.~(\ref{eta upper bound}).
%For $\eta \neq 0$, the upper bound is
%%Here I rewrite it in a simplified form
%\begin{align}
%%-\frac{\delta\chi^2(\hatm(\eta), \eta)}{\chi^2(\hatm(\eta), \eta)}
%%%= -\delta\eta
%%%    \frac{\dv{\eta} \chi^2(\hatm(\eta), \eta)}
%%%    {\chi^2(\hatm(\eta), \eta)}
%\delta\eta \frac{\hat{\vb{r}}_{\eta}^{\dagger} \inv{N(\eta)} \Nbar 
%    \inv{N(\eta)} \hat{\vb{r}}_{\eta} }
%    {\hat{\vb{r}}_{\eta}^{\dagger} \inv{N(\eta)} \hat{\vb{r}}_{\eta} }
%%\\
%\leq  \frac{\delta \eta}{\eta + \frac{\tau}{\max(N_f) -\tau}}
%\end{align}
%with
%$
%\vb{r}_{\eta} %= \vbd - P\hatm(\eta) 
%= \qty[ 1 - P\PPinv{\inv{N(\eta)}}\Pdagger \inv{N(\eta)}]\vbd
%\equiv \mathcal{P}_{\eta} \vbd
%$.
%To get the upper bound we treated $\vbd - P\hatm(\eta)$ as an arbitrary 
%vector in frequency domain, since we don't know how to calculate 
%$\mathcal{P}_{\eta}$ for $\eta \neq 0$, and it's hard to 
%analyze the projection matrix $\mathcal{P}_{\eta}$ in frequency space,
%as it contains $\PPinv{\inv{N(\eta)}}$.
%Note that we have to determine all of $\eta$ value before calculation, 
%because we don't want to keep the time ordered data in system RAM,
%so we need to somehow analytically analyze $\mathcal{P}_{\eta}$, and its behavior
%in frequency space.
%Unless $\vb{r}_{\eta}$ almost only has large noise modes,
%$\qty|\dv{\eta}\chi^2(\hatm(\eta),\eta)/\chi^2(\hatm(\eta),\eta)|$
%won't get close to the upper bound
%$1/\qty(\eta + \frac{\tau}{\max(N_f) -\tau})$.
%Based on the analysis in Section(\ref{intuitive interp}),
%for small $\eta$ the estimated map $\hatm(\eta)$ does not only focusing on 
%minimizing error $\vb{r}_{\eta}$ at low noise region.
%So we would expect that there would be a fair amount of low noise modes
%contribution in $\vb{r}_{\eta}$ especially for the first few $\eta$ values.
%Which means if we could somehow know the frequency distribution of 
%$\vb{r}_{\eta}$, we could tighten the boundary of
%$\qty|\dv{\eta}\chi^2(\hatm(\eta),\eta)/\chi^2(\hatm(\eta),\eta)|$,
%and get larger $\delta\eta$ value.
%This should make $\eta$ goes to $1$ faster, and yields the fewer $\eta$ parameters 
%we need.
%
%Also notice that the $\eta$ values determined from Eq.~(\ref{etai rule})
%%\begin{align}
%%\eta_i =\min \qty\bigg{1,\; \frac{\tau}{\max(\Nbar_f)} \qty(2^i -1) } 
%%\tag{\ref{etai rule}}
%%\end{align}
%are not dependent on any scanning information,
%it only depends on noise power spectrum $P(f)$, or noise covariance matrix $N$.
%In Appendix \ref{other cases} we would show 
%%Figure(\ref{large condi num 0.001}) and Figure(\ref{large condi num 10}) show
%two examples with same parameters as in Figure(\ref{f scan 0.1}) except 
%scanning frequency $f_{\text{scan}}$.%, in Figure(\ref{large condi num 0.001}) it
%%scans very slow and in Figure(\ref{large condi num 10}) it's very fast.
%%In these two cases our $\eta$ values based on Eq.~(\ref{etai rule}) are better
%%than manually selected values.
%%Based on these two results we know,
%It turns out the $\eta$ values should somehow depends
%on scanning scheme.
%Again that's because when we determine the upper bound% of 
%%$\dv{\eta} \chi^2(\hatm(\eta), \eta)$
%we treated
%$\vb{r}_{\eta}$ % = \vbd - P\hatm = \mathcal{P}_{\eta} \vbd$
%as an arbitrary vector, such that we lose all information related to scanning 
%scheme in the pointing matrix $P$.
%
%Even though the perturbation parameter $\eta$ get from Eq.~(\ref{etai rule}) are
%not the most optimal,
%it still performs much better than traditional conjugate gradient method under
%$1/f$ noise scenario without adding extra computational cost.
%The only extra free parameter added is to determine whether the error at
%current step $\vb{r}(\eta_i) = \qty||\vbb(\eta_i) - A(\eta_i) \vbm||$ is small
%enough such that we advance to next value $\eta_{i+1}$.
%Since it only takes in to account the noise information in $N$,
%but ignored all scanning information contained in pointing matrix $P$, because
%we are unable to analyze the structure of
%$\vb{r}_{\eta} = \vbd - P\hatm(\eta) = \mathcal{P}_{\eta} \vbd$ in frequency
%space.


%Also this analysis of $\eta$ value also explains why cooling parameters
%$\lambda=1/\eta$ in messenger field are chosen to be geometric series or
%\texttt{logspace} used in \cite{Huffenberger_2018}.

\section{Conclusions} \label{sec:conclusions} 

\kmh{We need some discussion of the things that haven't yet been demonstrated with the PCG, like multiple messenger fields.  Has the Kodi-Ramanah dual messenger field scheme been demonstrated in a PCG scheme by Papez?}

We presented a parameterized conjugate gradient method with parameter $\eta$ based on the idea of messenger field
separating the white noise out of noise covariance matrix.
Then we gave an analytical expression for $\eta$ series,
and showed that this method would not introduce extra computational cost than traditional conjugate method.

We tested this method under different power spectrum both apodized and non-apodized.
The results showed that this method is faster than traditional conjugate gradient method 
when there are significant amount of low frequency noise.
But it could be further improved if we could get more accurate estimation for Eq.~(\ref{exact eta}),
either before iteration or without using time ordered data during iteration.

Also note that we fixed preconditioner as $M = \Pdagger P$ during our calculation,
this parameterizing process could be applied to any preconditioner and possibly improve performance when 
there are significant amount of low frequency noise.

\citet{2018A&A...620A..59P} showed that the messenger field method solving Wiener
filter problem introduced by \cite{2013A&A...549A.111E} could also be written as 
parameterized conjugate gradient algorithm.
Then \citet{2017MNRAS.468.1782K} introduced dual messenger field method to Wiener filter.
If applying our idea to Wiener filter problem, hopefully, it may also bring improvements.













%% IMPORTANT! The old "\acknowledgment" command has be depreciated. It was
%% not robust enough to handle our new dual anonymous review requirements and
%% thus been replaced with the acknowledgment environment. If you try to 
%% compile with \acknowledgment you will get an error print to the screen
%% and in the compiled pdf.
\begin{acknowledgments}
BQ and KH are supported by NSF award 1815887.
\end{acknowledgments}

%% To help institutions obtain information on the effectiveness of their 
%% telescopes the AAS Journals has created a group of keywords for telescope 
%% facilities.
%
%% Following the acknowledgments section, use the following syntax and the
%% \facility{} or \facilities{} macros to list the keywords of facilities used 
%% in the research for the paper.  Each keyword is check against the master 
%% list during copy editing.  Individual instruments can be provided in 
%% parentheses, after the keyword, but they are not verified.

%\vspace{5mm}
%\facilities{HST(STIS), Swift(XRT and UVOT), AAVSO, CTIO:1.3m,
%CTIO:1.5m,CXO}

%% Similar to \facility{}, there is the optional \software command to allow 
%% authors a place to specify which programs were used during the creation of 
%% the manuscript. Authors should list each code and include either a
%% citation or url to the code inside ()s when available.

%\software{astropy \citep{2013A&A...558A..33A,2018AJ....156..123A},  
%          Cloudy \citep{2013RMxAA..49..137F}, 
%          Source Extractor \citep{1996A&AS..117..393B}
%          }

%% Appendix material should be preceded with a single \appendix command.
%% There should be a \section command for each appendix. Mark appendix
%% subsections with the same markup you use in the main body of the paper.

%% Each Appendix (indicated with \section) will be lettered A, B, C, etc.
%% The equation counter will reset when it encounters the \appendix
%% command and will number appendix equations (A1), (A2), etc. The
%% Figure and Table counter will not reset.


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%
% APPENDIX
%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\appendix
\section{The sequence of inverse cooling parameters} \label{appendix:eta calculation}

We know that the initial inverse cooling parameter $\eta_0 = 0$.  
What would be good value for the next parameter $\eta_1$?
To simplify notation, we use $\Neta$ to denote $N(\eta) = \tau I +  \eta \bar N$.
For some specific $\eta$ value, the minimum $\chi^2$ value is given by the optimized map
%The parameterized estimated map
$\hatm(\eta) = \PPinv{\inv{\Neta}} \Pdagger \inv{\Neta} \vbd$,
which minimizes
%\begin{align}
%\chi^2 (\vbm, \eta)
%= \qty(\vbd - P \vbm)^{\dagger} \inv{\Neta} (\vbd - P\vbm).
%\label{chi2 eta formula}
%\end{align}
\begin{align}
\begin{aligned}[b]
\chi^2 (\hatm(\eta), \eta)
&= \qty\big(\vbd - P \hatm(\eta))^{\dagger} \inv{\Neta} 
    \qty\big(\vbd - P\hatm(\eta)).
\label{chi2 eta formula}
\end{aligned}
\end{align}
We restrict to the case that the noise covariance matrix
$N$ is diagonal in the frequency domain, and represent the frequency-domain eigenvalues as $N_f$.

%Now let us see how $\chi^2(\hatm(\eta),\eta)$ changes as we change $\eta$.
%The relative decrease of $\chi^2(\hatm(\eta), \eta)$ at $\eta$ is defined as
%\begin{align}
%\begin{aligned}[b]
%&-\frac{\delta \chi^2(\hatm(\eta),\eta)} {\chi^2(\hatm(\eta), \eta)}\\
%=& 
%\delta \eta 
%\frac{\qty(\vbd - P\hatm(\eta))^{\dagger}
%    \inv{\Neta} \Nbar \inv{\Neta}
%    (\vbd - P\hatm(\eta)) 
%}
%{\qty\big(\vbd - P \hatm(\eta))^{\dagger} 
%    \inv{\Neta}
%    \qty\big(\vbd - P\hatm(\eta))
%}
%\end{aligned}
%\end{align}
%Here we put a minus sign in front of
%$\delta\chi^2(\hatm(\eta), \eta)/\chi^2(\hatm(\eta), \eta)$,
Let us first consider $\eta_1 = \eta_0 + \delta\eta = \delta\eta$
such that $\eta_1 = \delta \eta$ is very small quantity, $\delta \eta \ll 1$.
Since $\hatm(\eta)$ minimizes $\chi^2(\hatm(\eta),\eta)$, we have 
$\pdv{\hatm} \chi^2(\hatm(\eta), \eta) = 0$,
and using chain rule
\begin{align}
\dv{\eta} \chi^2(\hatm(\eta), \eta) = \pdv{\eta} \chi^2(\hatm(\eta), \eta) 
= - \qty(\vbd - P\hatm(\eta))^{\dagger} \inv{\Neta} \Nbar \inv{\Neta}
    (\vbd - P\hatm(\eta)) \label{d chi2}
\end{align}
Then the fractional decrease of $\chi^2(\hatm(0),0)$ from $\eta_0= 0$ to 
$\eta_1 = \delta \eta$ is
\begin{align}
-\frac{\delta \chi^2(\hatm(0), 0)}{\chi^2(\hatm(0), 0)} 
= - \delta \eta \frac{\dv{\eta} \chi^2(\hatm(0),0)}{\chi^2(\hatm(0),0)}
&= \delta \eta 
\frac{1}{\tau}
\frac{\qty(\vbd - P\hatm(0))^{\dagger} \Nbar  (\vbd - P\hatm(0)) }
    {\qty\big(\vbd - P \hatm(0))^{\dagger} \qty\big(\vbd - P\hatm(0))}
%\\
%& \leq  \frac{\delta \eta}{\tau} \max(\Nbar_f)
\end{align}
Here we put a minus sign in front of this expression such that it's 
non-negative, and use $N_{\eta=0} = \tau I$ at the second equality.
%\kmh{This statement is kind of non trivial...  I think you need to explain more the step $(\tau I + \delta \eta \bar N)^{-1} \approx \tau I - \delta \eta \bar N$. (I think that's right, even if you don't restrict to the frequency diagonal case.)}
Since it is hard to analyze $\vbd - P\hatm$ under frequency domain,
we treat it as an arbitrary vector, then the least upper bound is given by
\begin{align}
-\frac{\delta \chi^2(\hatm(0), 0)}{\chi^2(\hatm(0), 0)} 
\leq \frac{\delta \eta} {\tau} \max(\Nbar_f)
\end{align}
where $\max(\Nbar_f)$ is the maximum eigenvalue of $\Nbar$.
Here if we assume that initial $\chi^2$ value $\chi^2(\hatm(0),0)$ is much
larger than final value $\chi^2(\hatm(1),1)$,
$\chi^2(\hatm(0), 0) \gg \chi^2(\hatm(1),1)$,
then we would expect
\begin{align}
-\frac{\delta\chi^2(\hatm(0),0)}{\chi^2(\hatm(0),0)}
= 1 - \frac{\chi^2(\hatm(1),1)}{\chi^2(\hatm(0),0)}
\approx 1^-
\end{align}
The upper bound is strictly smaller than 1.
Ideally, if
$\delta \chi^2(\hatm(0),0) = \chi^2(\hatm(1), 1) - \chi^2(\hatm(0),0)$,
then it would get close to the final $\chi^2$ at next iteration,
but we do not know the final $\chi^2(\hatm(1),1)$.
So we want $\qty|\frac{\delta\chi^2(\hatm(0),0)}{\chi^2(\hatm(0),0)}|$ to be as
large as possible, so it could converge fast, but subject to another constraint that the least upper bound cannot exceed 1.
Therefore we can choose $\delta \eta$ such that the least upper bound is equal to 1.
%we could set its upper bound equal to 1, $\delta \eta \max(\Nbar_f) / \tau = 1$ \kmh{explain this reasoning a bit better}.
%This gives
Thus we choose
\begin{equation}
\eta_1 \equiv \frac{\tau}{\max(\Nbar_f)} = \frac{\min(N_f)}{\max(N_f) - \min(N_f)}.
\end{equation}
Here $N_f$ and $\Nbar_f$ are the eigenvalues of $N$ and $\Nbar$ in the frequency
domain.
If the condition number of noise covariance matrix
$\kappa(N) = \max(N_f)/\min(N_f) \gg 1$,
then $\eta_1 \approx \inv{\kappa} (N)$.

What about the other parameters $\eta_m$ with $m > 1$?
We use a similar analysis,
letting $\eta_{m+1} = \eta_m + \delta \eta_m$ with a small $\delta\eta_m \ll 1$,
and set the least upper bound of relative decrease equal to 1.
%See Appendix \ref{derive other etas} for detailed derivation.
%We would get
\begin{align}
-\frac{\delta \chi^2(\hatm(\eta_m), \eta_m)}{\chi^2(\hatm(\eta_m), \eta_m)}  
=& \delta\eta_m
\frac{\qty(\vbd - P\hatm(\eta_m))^{\dagger}
    \inv{N_{\eta_m}} \Nbar \inv{N_{\eta_m}}
    (\vbd - P\hatm(\eta_m))
}
{\qty\big(\vbd - P \hatm(\eta_m))^{\dagger}
    \inv{N_{\eta_m}}
    \qty\big(\vbd - P\hatm(\eta_m))
}
\label{dchi2 chi2}
\\
\leq & \delta \eta_m\, \max\qty(\frac{\Nbar_f}{\tau + \eta_m \Nbar_f})
\end{align}
The upper bound in the second line is a little bit tricky.
Both matrix $\Nbar$ and $\inv{N}_{\eta_m}$ 
can be simultaneously diagonalized in frequency space.
For each eigenvector $\vb{e}_f$,
the corresponding eigenvalue of the matrix on the numerator
$\inv{N}_{\eta_m} \Nbar \inv{N}_{\eta_m}$
is
$\lambda_f = \Nbar_f (\tau + \eta_m \Nbar_f)^{-2}$,
and the eigenvalue for matrix on the denominator
$\inv{N}_{\eta_m}$
is
$\gamma_f = (\tau + \eta_m \Nbar_f)^{-1}$.
Their eigenvalues are related by
$\lambda_f = [{\Nbar_f}/{(\tau + \eta_m \Nbar_f)}] \gamma_f$.
For any vector $\vb{v} = \sum_f \alpha_f \vb{e}_f$, we have
\begin{equation}
  \frac{\vb{v}^{\dagger} \inv{N}_{\eta_m} \Nbar \inv{N}_{\eta_m} \vb{v}}
{\vb{v}^{\dagger} \inv{N}_{\eta_m} \vb{v}}
= \frac{\sum_f \alpha_f^2 \lambda_f}{\sum_f \alpha_f^2 \gamma_f}
= \frac{\sum_f \alpha_f^2 \gamma_f \Nbar_f/(\tau + \eta_m \Nbar_f)}
{\sum_f \alpha_f^2 \gamma_f}
\leq \max \qty( \frac{\Nbar_f}{\tau + \eta_m \Nbar_f}).
\end{equation}

Similarly, we could set the least upper bound equal to 1.
Then we get
\begin{align}
\delta \eta_m 
= \min \qty(\frac{\tau + \eta_m \Nbar_f}{\Nbar_f})
= \eta_m + \frac{\tau }{\max(\Nbar_f)}.
\end{align}
Therefore 
\begin{align}
\eta_{m+1} = \eta_m + \delta\eta_m = 2\eta_m + \frac{\tau }{\max (\Nbar_f)}
\end{align}
The final term ${\tau }/{\max (\Nbar_f)} = \eta_1$ becomes subdominant after a few terms, and we see that the $\eta_m$ increase like a geometric series. 
%\footnote{
Here we assumed that
$\chi^2(\hatm(\eta_m),\eta_m) \gg \chi^2(\hatm(1),1)$,
which we expect it to be satisfied for our assumed $ \eta_m \ll 1$. 
Since the final result %Eq.~(\ref{etai rule})
is geometric series,
only the last few $\eta_m$ values fail to be much smaller than 1.
%}

If written in the form $\eta_{m+1} + {\tau }/{\max(\Nbar_f)}
= 2 \qty( \eta_m + {\tau}/{\max(\Nbar_f)})$
it's easy to see that for $m \geq 1$,
$\eta_{m} + {\tau }/{\max(\Nbar_f)}$ forms a geometric series
\begin{align}
\eta_m +  \frac{\tau }{\max(\Nbar_f)}
=\qty(\eta_1 + \frac{\tau }{\max(\Nbar_f)}) 2^{m-1}
=\frac{\tau}{\max(\Nbar_f)} 2^m
\end{align}
where we used $\eta_1 = {\tau}/{\max(\bar{N}_f)}$.
Note that $m = 0$ and $\eta_0 = 0$ also satisfy this expression and we've got
final expression for all $\eta_m$
\begin{align}
\eta_m =\min \qty\bigg{1,\; \frac{\tau}{\max(\Nbar_f)} \qty(2^m -1) }
\end{align}
Here we need to truncate the series when $\eta_m > 1$.




 






%% For this sample we use BibTeX plus aasjournals.bst to generate the
%% the bibliography. The sample631.bib file was populated from ADS. To
%% get the citations to show in the compiled file do the following:
%%
%% pdflatex sample631.tex
%% bibtext sample631
%% pdflatex sample631.tex
%% pdflatex sample631.tex

\bibliography{references.bib}{}
\bibliographystyle{aasjournal}

%% This command is needed to show the entire author+affiliation list when
%% the collaboration and author truncation commands are used.  It has to
%% go at the end of the manuscript.
%\allauthors

%% Include this line if you are using the \added, \replaced, \deleted
%% commands to see a summary list of all changes at the end of the article.
%\listofchanges

\end{document}

% End of file `sample631.tex'.
