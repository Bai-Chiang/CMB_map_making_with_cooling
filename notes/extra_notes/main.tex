\documentclass[11pt, letterpaper]{article}
\usepackage{geometry}
\usepackage[utf8]{inputenc}
\usepackage[tbtags]{amsmath}
\usepackage{amssymb}
\usepackage{gensymb}
\usepackage{physics}
\usepackage[makeroom,Smaller]{cancel}
\usepackage[dvipsnames]{xcolor}
\usepackage{tikz}

\usepackage{hyperref}
\hypersetup{
    colorlinks=true,
    linkcolor=black,
}

\usepackage{aas_macros}

\usepackage{graphicx}
\graphicspath{ {./images/} }
\usepackage{subcaption}

% tikzplotlib
\usepackage{pgfplots}
%\DeclareUnicodeCharacter{2212}{âˆ’}
\usepgfplotslibrary{groupplots,dateplot}
\usetikzlibrary{patterns,shapes.arrows}
\pgfplotsset{compat=newest}

% footnote
\renewcommand{\thefootnote}{\fnsymbol{footnote}}

% Operators
\DeclareMathOperator*{\argmax}{arg\,max}
\DeclareMathOperator*{\argmin}{arg\,min}
\DeclareMathOperator*{\diag}{diag}
\newcommand{\exptval}[1]{\mathbb{E}\left[ #1 \right]}
\newcommand{\Var}[1]{\text{Var}\qty[#1]}
\newcommand{\Cov}[1]{\text{Cov}\qty[#1]}

% variables
\newcommand{\bb}[1]{\mathbb{#1}}
\newcommand{\vbd}{\vb{d}}
\newcommand{\vbm}{\vb{m}}
\newcommand{\vbep}{\vb*{\varepsilon}}
\newcommand{\vbn}{\vb{n}}
\newcommand{\vbb}{\vb{b}}
\newcommand{\inv}[1]{#1^{-1}}
\newcommand{\noisevar}{\Sigma_{\vbn}}
\newcommand{\hatm}{\vb{\hat{m}}}
\newcommand{\Pdagger}{P^{\dagger}}
\newcommand{\Nbar}{\bar{N}}
\newcommand{\PPinv}[1]{\inv{\qty(\Pdagger #1 P)}}
\newcommand{\Neta}{N_{\eta}}


%\title{Perturbative Approach to Solve the Map-Making Equation}
\title{Extra Notes}
\author{Bai-Qiang Qiang}
\date{\today}

\begin{document}
\maketitle

%\section{Introduction}
%The cosmic microwave background is an electromagnetic radiation coming from early
%stage of our Universe. Based on the hot Big Bang model, before the recombination
%epoch, the photons were tightly coupled with free electrons and protons via 
%Thomson scattering. As the universe expanded and cooled down, free electrons and
%protons combined into neutral hydrogen atom.  This process is called 
%recombination in cosmology. Shortly after this epoch, the photons could
%propagate freely and would not be scattered by charged particles. Today, we receive
%the photons produced at that time and called them the cosmic microwave background
%radiation. Studying these photons coming from early universe can help us
%to constrain theoretical model and cosmological parameters
%\cite{10.1093/ptep/ptaa104}.
%The next generation CMB observations will have much higher resolution and 
%generate more data. So we need an efficient way to process the data.
%One step of the the processing is map making, which gives an estimated map based on the raw observation data.
%
%Recently Elsner and Wandelt \cite{2013A&A...549A.111E} introduced a new
%method called messenger field to solve Wiener filter, and then this technique
%was being applied to map making equation by Huffenberger and Naess \cite{Huffenberger_2018}.
%It has been shown that this messenger field method is equivalent to applying a
%preconditioner to the original problem and introducing an extra cooling parameter $\lambda$, but whether this cooling parameter will boost performance compare to
% the (traditional) conjugate gradient method is still controversial \cite{2018A&A...620A..59P}. 
%Here I give a detailed analysis of this parameter and show that
%it may improve performance under some circumstances, if we properly choose
%its values.

\section{Map Making Setup}

The map making procedure could be summarized in equation
\begin{equation}
\vbd = P\vbm + \vbn \label{map making model}
\end{equation}
where $\vbd$, $P$, $\vbm$, $\vbn$ are time-ordered data (TOD), pointing matrix,
CMB map, and noise.
The time-ordered data we collected is given by map signal $P\vbm$ plus noise 
$\vbn$.
The pointing matrix $P$ acting on the map gives the signal of the map at some specific 
position of sky where telescope is pointing at.
Here we could assume that the noise has zero mean $\ev{\vbn} = \vb{0}$,
since if it is not zero, we can always subtract its mean value to make it zero.
And noise covariance matrix could be written as $N = \ev{\vbn \vbn^{\dagger}}$.

As we can see the map making model Eq.(\ref{map making model}) mathematically 
is a standard linear regression problem,
with \textit{design matrix} being pointing matrix $P$, and \textit{regression
coefficients} are $\vbm$.
Naturally, we want to estimate linear regression coefficients $\vbm$,
with \textit{generalized least square} (GLS) technique.
The noise $\vbn$ is \textit{heteroscedastic}: the variances $N$ are 
different for various frequencies.  Usually detectors have a $1/f$ noise pattern
\cite{1997PhRvD..56.4514T}.
The \textit{generalized least square} (GLS) will provide better estimation 
than \textit{ordinary least square} (OLS) method, because the data is
heteroscedastic so we would like to focusing on fitting the data with lower 
noise.

The GLS estimated map $\hatm$ is given by
\begin{equation}
\hatm = \argmin_{\vbm} (\vbd - P\vbm)^{\dagger} N^{-1} (\vbd - P \vbm) 
\label{GLS estimator equation}
\end{equation}
and we could define 
\begin{align}
\chi^2(\vbm) & \equiv (\vbd - P\vbm)^{\dagger} N^{-1} (\vbd - P \vbm).
\label{chi2 formula}
\end{align}
Therefore the estimated map $\hatm$ is the one that minimizes $\chi^2(\vbm)$.
To find out the expression for $\hatm$, we first take derivative with respect to 
vector $\vbm$
\begin{align}
\begin{aligned}[b]
\pdv{\vbm} \chi^2(\vbm)
&= \pdv{\vbm} (\vbd - P\vbm)^{\dagger} N^{-1} (\vbd - P \vbm)
\\
&= \pdv{\vbm} \qty\Big(
    \vbd^{\dagger} N^{-1} \vbd 
    - \vbd^{\dagger} N^{-1} P \vbm 
    - \vbm^{\dagger}\Pdagger N^{-1}\vbd 
    + \vbm^{\dagger} \Pdagger N^{-1}P\vbm
)
\\
&= -2\Pdagger N^{-1} \vbd + 2 \Pdagger N^{-1} P \vbm
\label{dchi2/dm}
\end{aligned}
\end{align}
then set it equal to zero $\pdv{\vbm} \chi^2(\hatm) = 0$.
We get the \textit{map making equation}
\begin{align}
\hatm = \PPinv{N} \Pdagger \inv{N} \vbd \label{map making equation}
\end{align}
This is also called COBE method for map making.

\section{Convient Properties for Map-making Equation}

\subsection{Unbiased linear estimator}
A linear estimator means $\hatm$ could be written as $\hatm = W\vbd$, that is, 
it is a linear combination of $\vbd$.
We say that the estimator is unbiased if
\begin{align}
\begin{aligned}[b]
&\mathrel{\phantom{\Rightarrow}}\ev{\hatm} = \vbm \\
&\Rightarrow\ev{W \vbd} = \vbm\\
&\Rightarrow\ev{W(P\vbm + \vbn) } = \vbm \\
&\Rightarrow WP = I.
\end{aligned}
\end{align}
At last step we used the property  $\ev{\vbn} = 0$.
The generalized least square estimator matrix 
$W = \PPinv{N} \Pdagger \inv{N}  $ satisfies the condition $WP=I$.
Therefore $\hatm$ is an unbiased estimated map.


\subsection{Minimum variance linear estimators under the unbiased constraint} \label{minimize variance}
The covariance of the estimator $\hatm = W \vbd$ is
\begin{align}
\begin{aligned}[b]
\Cov{\hatm} &= \Cov{W\vbd} 
\\ 
&= \Cov{WP\vbm + W\vbn} 
\\ 
&= \Cov{W\vbn} 
\\ 
&= W N W^{\dagger} \label{covariance hatm}
\end{aligned}
\end{align}

Here we use a trick \cite{weighted_and_GLS} and consider the matrix $W = W_{GLS} + W'$ where
$W_{GLS} = \PPinv{N} \Pdagger \inv{N} $ is the matrix for GLS estimation,
and in order to satisfy the condition $WP = I$, we should have $W'P = 0$.
Then the covariance matrix
\begin{align}
\begin{aligned}[b]
\Cov{\hatm }
&= W_{GLS} N W_{GLS}^{\dagger} + W' N W'^{\dagger} 
    + W_{GLS} N W'^{\dagger} + W' N W_{GLS}^{\dagger}
\\
&= \PPinv{N} + W' N W'^{\dagger} 
    + \PPinv{N} \Pdagger W'^{\dagger} + W' P \PPinv{N}
\\
&= \PPinv{N} + W' N W'^{\dagger} 
\end{aligned}
\end{align}
where the last line used condition $W'  P = 0$.

The variance $\Var{\hatm_i}$ is diagonal elements of covariance matrix
$\Cov{\hatm}$
\begin{align}
\begin{aligned}[b]
\Var{\hatm_i}
&= \qty\Big{ \PPinv{N}}_{ii} + \qty\big{W' N W'^{\dagger}}_{ii}
\\
&= \qty\Big{ \PPinv{N}}_{ii} + W'_{i,:} N W'^{\dagger}_{i,:}
\end{aligned}
\end{align}
where $W'_{i,:}$ is the $i^{th}$ row vector of $W'$.
Since the noise covariance matrix $N$ is a positive semi-definite matrix,
therefore $W'_{i,:} N W'^{\dagger}_{i,:} \geq 0$.
If $W' = 0$ Then we have $W = W_{GLS}$, and the variance $\Var{\hatm_i}$ would have 
its minimum variance. 

\subsection{Minimize mean square error under constrain of unbiased linear 
estimator}
The error is defined as the difference between estimated map and real one
\begin{align}
\begin{aligned}[b]
\vbep &\equiv \hatm - \vbm
\\
&= W\vbd - \vbm
\\
&= (WP-I)\vbm + W\vbn
\\
&= W\vbn
\end{aligned}
\end{align}
where the last line used relation $WP=I$ for unbiased estimator $W$. 

Now we need to minimize mean square error
\begin{align}
\begin{aligned}[b]
\ev{\vbep^{\dagger} \vbep }
&= \ev{\Tr(\vbep \vbep^{\dagger})}
\\
&=\ev{\Tr(W\vbn \vbn^{\dagger} W^{\dagger}) }
\\
&= \Tr(W N W^{\dagger})
\\
&= \Tr(\Cov{\hatm})\\
&= \sum_i \Var{\hatm_i}
\end{aligned}
\end{align}
In the second line we used property $\vbep^{\dagger} \vbep$ is a scalar,
so $\vbep^{\dagger} \vbep = \Tr(\vbep^{\dagger} \vbep) 
= \Tr(\vbep \vbep^{\dagger})$.
In the fourth line the trace is a linear operation 
and $\ev{\vbn \vbn^{\dagger} } = N$. 
The fifth line comes from Eq.(\ref{covariance hatm}).
In Section \ref{minimize variance} we have shown that the generalized least 
square matrix $W_{GLS}$ minimizes $\Var{\hatm_i}$ for each $i$, 
therefore it also minimizes the mean square error
$\ev{\vbep^{\dagger} \vbep} = \sum_i \Var{\hatm_i}$. 

\subsection{Maximum likelihood estimator for Gaussian noise}
The previous properties does not depends on the noise distribution, 
if we assume that the noise has a multivariate normal distribution,
$\vbn \sim \mathcal{N}(0, N)$ with mean $0$ covariance $N$,
its likelihood function will be
\begin{align}
L\qty(\vbd;\vbm) = \frac{1}{\sqrt{(2\pi)^n \abs{N}}} 
    \exp(-\frac{1}{2} \qty(\vbd - P\vbm)^{\dagger} N \qty(\vbd - P\vbm))
\end{align}
and log-likelihood
\begin{align}
\log\qty(L\qty(\vbd;\vbm))
= -\frac{1}{2} \qty(\vbd - P\vbm)^{\dagger} N \qty(\vbd - P\vbm) + \text{cont.}  
\end{align}
Maximizing this log-likelihood function with respect to $\vbm$, is equivalent
to minimize $\chi^2(\vbm) =\qty(\vbd - P\vbm)^{\dagger} N \qty(\vbd - P\vbm)$,
which is $\hatm$.

\section{Solving the Map Making Equation}

The map making equation Eq.(\ref{map making equation}) derived from Generalized
Least Square estimation,
\begin{align}
\qty(\Pdagger \inv{N}  P) \hatm = \Pdagger \inv{N} \vbd \label{map making eq}
\end{align}
If we define $A = \Pdagger \inv{N} P$ and $\vbb = \Pdagger \inv{N} \vbd$,
then it could be written as $A\hatm = \vbb$.

Based on current computation power, it is impossible to solve $\hatm$
by calculating $\hatm = \PPinv{\inv{N}} \Pdagger \inv{N} \vbd$ directly,
since the noise covariance matrix $N$ is sparse in frequency domain,
and pointing matrix $P$ is sparse in (time by pixel) domain.  In experiments currently under design, there may be $\sim 10^{16}$ time samples and $\sim 10^{9}$ pixels, so these matrix inversions are intractable.
It is impossible to do these matrix multiplication directly and then take
inverse.
However, for a vector with size of map $\hatm$, we could calculate
$\Pdagger \inv{N} P \hatm = A\hatm$ by first taking Fourier transform $P\hatm$
then inverse Fourier transform $\inv{N}P\hatm$.
This means it can be solved by the conjugate gradient method.


\subsection{Preconditioner}
To improve the performance of the conjugate gradient method,
we could apply a preconditioner $M$ to original problem $A\hatm = \vbb$,
which then becomes $\inv{M}A\hatm = \inv{M} \vbb$.
The preconditioner should reduce the condition number of original problem,
so that the conjugate gradient method will converge faster.
We want the preconditioner to capture as much information as possible from 
matrix $A$, but still keep it relative easy to calculate $\inv{M}$.
For example, if $M = A$, $\inv{M} A \hatm = \inv{M} \vbb$ would be solved
immediately, but $\inv{M}$ will be extremely difficult to calculate.
We could simply choose $M = \Pdagger P$,
and the operation $\inv{M} \vbm = \PPinv{} \vbm$ is the average over each pixel 
of map $\vbm$.

For the conjugate gradient method, we need an initial guess map $\hatm_0$. 
We can use zero vector $\hatm_0 = \vb{0}$ as initial guess,
but the simple binned map $\hatm_0 = \PPinv{} \Pdagger \vbd$ is a better 
choice (it is a the solution for white noise case $N \propto I$).
(Pape\v{z} et al. 2018\cite{2018A&A...620A..59P}) showed that using 
$\hatm_0$ as initial guess could improve performance significantly compare to
zero vector $\vb{0}$ in come cases.
As stated before, we can calculate $\PPinv{}$ acting on any map-sized object,
and $\Pdagger \vbd$ is indeed a map size object, 
so we could obtain simple binned map by calculating
$\hatm_0 = \PPinv{} \Pdagger \vbd$ directly.

For the conjugate gradient method with simple preconditioner $M = \Pdagger P$,
we have all we need.  Next we only need to use conjugate gradient algorithm solve the problem.

\subsection{Parameterized Conjugate Gradient Method}

The above results have appeared in the literature.  Here we now show that we 
can improve performance in some cases by using a parameterized version of the map making equation Eq.(\ref{map making eq}).
The idea is that map making equation Eq.(\ref{map making eq}) is hard to solve
due to noise covariance matrix is sandwiched between $\Pdagger P$.
But if noise covariance matrix $N$ is proportional to identity matrix $I$, 
then its solution is given by simple binned map
$\vbm_0 = \inv{\qty(\Pdagger P)} \Pdagger \vbd$,
which could be solved directly. 
We can parameterize the noise covariance matrix $N$ with a parameter $\eta$,
such that initially $\eta = \eta_i$ we have $N\qty(\eta_i) \propto I$ 
and in the end $\eta = \eta_f$ and $N\qty(\eta_f) \propto N$,
such that the final solution is what we want.
We expect that the parameterized noise covariance matrix $N(\eta)$
would connect our initial guess $\hatm_0$ and final solution $\hatm$ as we 
change $\eta$ from $\eta_i$ to $\eta_f$.

Now instead of Eq.(\ref{map making eq}), we are solving
\begin{align}
\qty(\Pdagger \inv{N(\eta)} P)\, \hatm(\eta) 
= \Pdagger \inv{N(\eta)} \vbd \label{map making para}
\end{align}

Now question is how to find $N(\eta)$ such that $N(\eta_i) \propto I$
and $N (\eta_f) \propto N$?
Since the non-white noise part of $N$ is the difficult portion,
we could think of it as a perturbation term, which adds upon the white noise.
Initially the covariance is homoscedastic and solution is given by $\hatm_0$,
then we gradually add extra noise into this equation by changing $\eta$.
At the end when $\eta=\eta_f$ we are solving equation Eq.(\ref{map making eq}).

Therefore we could separate noise covariance matrix into two parts
$N = \tau I + \Nbar$ where $\tau$ is the minimum eigenvalue of $N$. 
Then we define $N(\eta) = \tau I + \eta \Nbar$, 
with parameter $\eta$ represents the degree of \textit{heteroscedasticity} which satisfies $\eta_i = 0$ and $\eta_f=1$.

Eq.(\ref{map making para}) then becomes
\begin{align}
\qty(\Pdagger \inv{(\tau I + \eta \Nbar)}P) \, \hatm(\eta) 
= \Pdagger \inv{\qty(\tau I + \eta \Nbar)} \vbd  
\end{align}

We require $\eta$ being monotonically increase
series $0 = \eta_0 < \eta_1 < \cdots < \eta_n = 1$.
For some specific $\eta_m$, we use conjugate gradient method to solve equation 
$\qty(\Pdagger \inv{N(\eta_m)} P)\, 
\hatm(\eta_m) = \Pdagger \inv{N(\eta_m)} \vbd$
with simple preconditioner $\Pdagger P$,
and using $\hatm(\eta_{m-1})$ as the initial value.
The initial guess is $\hatm(\eta_0) = \vbm_0 = \PPinv{} \Pdagger \vbd$.


\subsubsection{Choosing parameters $\eta$}
The next question is how we choose these monotonically increasing parameters
$\eta$. 
If we choose these parameters inappropriately, it would only makes it converge
slower.
Also we want to determine $\eta_1, \cdots, \eta_{n-1}$ before starting conjugate
gradient iteration.
That's because time ordered data $\vbd$ is very large,
and we don't want to keep it in the system RAM during calculation.
If $\eta_1, \cdots, \eta_{n-1}$ could be determined before the iterations, 
then we can first calculate $\Pdagger \inv{N(\eta)} \vbd$ for each $\eta_m$
and store these map-sized objects in RAM,
instead of the entire time-ordered data $\vbd$.

First let us try to find out our starting point $\eta_1$.
What would be good value for $\eta_1$?

Here to simplify notation, I will use $\Neta$ to denote $N(\eta)$.
The estimated map
$\hatm(\eta) = \PPinv{\inv{\Neta}} \Pdagger \inv{\Neta} \vbd$
which minimizes
\begin{align}
\chi^2 (\vbm, \eta)
= \qty(\vbd - P \vbm)^{\dagger} \inv{\Neta} (\vbd - P\vbm).
\label{chi2 eta formula}
\end{align}
For some specific $\eta$ value, the minimum $\chi^2$ value is given by
\begin{align}
\begin{aligned}[b]
\chi^2 (\hatm(\eta), \eta)
&= \qty\big(\vbd - P \hatm(\eta))^{\dagger} \inv{\Neta} 
    \qty\big(\vbd - P\hatm(\eta))
\\
& = \vbd^{\dagger} \qty[\inv{\Neta}
    - \inv{\Neta}P\inv{\qty[
        \Pdagger \inv{\Neta} P
    ]}\Pdagger\inv{\Neta}]\vbd
\end{aligned}
\end{align}
Now let us see how $\chi^2(\hatm(\eta),\eta)$ changes as we change $\eta$.
\begin{align}
\begin{aligned}[b]
\dv{\eta} \chi^2(\hatm(\eta),\eta) 
&= \dv{\eta}(\vbd^{\dagger}\inv{\Neta}\vbd)
    - \dv{\eta}(
        \vbd^{\dagger}\inv{\Neta}P 
        \PPinv{\Neta} 
        \Pdagger \inv{\Neta} \vbd
    )
\\
&= \vbd^{\dagger} \inv{\Neta} \bigl[
    - \Nbar
    + \Nbar \inv{\Neta}P\PPinv{\Neta}\Pdagger
    \\
    &\phantom{= \vbd^{\dagger} \inv{\Neta} \bigl[}
    -P\PPinv{\inv{\Neta}} \Pdagger \inv{\Neta} \Nbar \inv{\Neta}
        P \PPinv{\inv{\Neta}}\Pdagger
    \\
    &\phantom{= \vbd^{\dagger} \inv{N_{\eta}} \bigl[}
    + P \PPinv{\inv{\Neta}} \Pdagger \inv{\Neta} \Nbar
    \big] \inv{\Neta} \vbd
\end{aligned}
\end{align}
Simplify this expression with identity
$\hatm = \PPinv{\inv{\Neta}} \Pdagger \inv{\Neta}\vbd$, and it yields
\begin{align}
\dv{\eta} \chi^2(\hatm(\eta), \eta) 
&= - \qty(\vbd - P\hatm(\eta))^{\dagger} \inv{\Neta} \Nbar \inv{\Neta} 
    (\vbd - P\hatm(\eta)) \label{d chi2}.
\end{align}
Also notice that
$\dv{\eta} \chi^2(\hatm(\eta), \eta) = \pdv{\eta} \chi^2(\hatm(\eta), \eta)$ (the total derivative is the partial derivative),
because by the definition of $\hatm(\eta)$ it minimize
$\chi^2(\vbm, \eta)$ for some fixed $\eta$ value,
implies $\pdv{\vbm} \chi^2(\hatm(\eta), \eta) = 0$.


To further simplify the analysis, let's assume that the noise covariance matrix
$N = \ev{\vbn\vbn^{\dagger}}$ is diagonal in the frequency domain.
Therefore $\Nbar$ and $\Neta$ are also diagonal in the frequency domain by
definition, and all the diagonal elements are greater than or equal to zero,
because covariance matrix is positive semi-definite.
Also, we can conclude that matrix
$\inv{\Neta} \Nbar \inv{\Neta}$ is positive semi-definite matrix.
Based on Eq. (\ref{d chi2}), we know that
$\dv{\eta} \chi^2({\hatm(\eta), \eta}) \leq 0$,
so $\chi^2(\hatm(\eta), \eta)$ is always decreasing
as $\eta$ changes from $0$ to $1$.

The fractional decrease of $\chi^2(\hatm(\eta), \eta)$ at $\eta$ is defined as
\begin{align}
\begin{aligned}[b]
-\frac{\delta \chi^2(\hatm(\eta),\eta)} {\chi^2(\hatm(\eta), \eta)}
&=
-\delta \eta \frac{1}{\chi^2(\hatm(\eta), \eta)} \, 
\dv{\eta} \chi^2(\hatm(\eta), \eta) 
\\&= 
\delta \eta 
\frac{\qty(\vbd - P\hatm(\eta))^{\dagger}
    \inv{\Neta} \Nbar \inv{\Neta}
    (\vbd - P\hatm(\eta)) 
}
{\qty\big(\vbd - P \hatm(\eta))^{\dagger} 
    \inv{\Neta}
    \qty\big(\vbd - P\hatm(\eta))
}
\end{aligned}
\end{align}
Here we put a minus sign in front of
$\delta\chi^2(\hatm(\eta), \eta)/\chi^2(\hatm(\eta), \eta)$,
such that it's non-negative.
If we choose $\eta_1 = \eta_0 + \delta\eta = \delta\eta$
such that $\eta_1 = \delta \eta$ is very small quantity.
Then the fractional decrease from $\eta_0= 0$ to $\eta_1 = \delta \eta$ is
\begin{align}
\begin{aligned}[b]
-\frac{\delta \chi^2(\hatm(0), 0)}{\chi^2(\hatm(0), 0)} 
&= \delta \eta 
\frac{1}{\tau}
\frac{\qty(\vbd - P\hatm(0))^{\dagger} \Nbar  (\vbd - P\hatm(0)) }
    {\qty\big(\vbd - P \hatm(0))^{\dagger} \qty\big(\vbd - P\hatm(0))}
%\\
%& \leq  \frac{\delta \eta}{\tau} \max(\Nbar_f)
\label{chi2 fractional decrease}
\end{aligned}
\end{align}
where we used the property $N_{\eta=0} = \tau I$.

We want $\qty|\delta \chi^2(\hatm(\eta_0),\eta_0)| = \chi^2(\hatm(\eta_0),\eta_0)) - \chi^2(\hatm(\eta_1), \eta_1)$
to be large such that it could converge fast.
Which means $\chi^2(\hatm(\eta_1), \eta_1)$ is much smaller than $\chi^2(\hatm(\eta_0), \eta_0)$,
or $\chi^2(\hatm(\eta_1), \eta_1) \ll \chi^2(\hatm(\eta_0), \eta_0)$.
Then we would expect
\begin{align}
-\frac{\delta\chi^2(\hatm(0),0)}{\chi^2(\hatm(0),0)}
= 1 - \frac{\chi^2(\hatm(\eta_1),\eta_1)}{\chi^2(\hatm(0),0)}
\approx 1^-
\label{dchi2/chi2 0}
\end{align}
The upper bound is strictly smaller than 1.
Now we could use Eq.(\ref{chi2 fractional decrease}) and let it equal to $1$, then
$\delta\eta=-\chi^2(\hatm(\eta_0), \eta_0)/\dv{\eta}\chi^2(\hatm(\eta_0),\eta_0)$.
However if we apply this idea to $\eta_{m+1} = \eta_m + \delta \eta_m$ with $m \geq 1$, we would get
\begin{align}
\delta\eta_m = -\chi^2(\hatm(\eta_m), \eta_m)/\dv{\eta}\chi^2(\hatm(\eta_m),\eta_m).
\label{delta eta update}
\end{align}
As mentioned before, we need to determine the entire series $\qty{\eta_i}$ before conjugate gradient iterations,
and we could not calculate $\hatm(\eta_m)$ directly because of the difficulty of matrix inversions.
Therefore we could not get $\delta \eta_m$ values in advance.
That means we need to find another approach.

Let us go back to Eq.(\ref{chi2 fractional decrease}).
Since it is hard to analyze $\vbd - P\hatm(\eta)$ under frequency domain,
we treat it as an arbitrary vector, then the least upper bound of Eq.(\ref{chi2 fractional decrease}) is given by
\begin{align}
-\frac{\delta \chi^2(\hatm(\eta_0), \eta_0)}{\chi^2(\hatm(\eta_0), \eta_0)} 
\leq \frac{\delta \eta} {\tau} \max(\Nbar_f)
\label{dchi2/chi2 upper bound}
\end{align}
where $\max(\Nbar_f)$ is the maximum eigenvalue of $\Nbar$.
We want $ -\frac{\delta \chi^2(\hatm(\eta_0), \eta_0)}{\chi^2(\hatm(\eta_0), \eta_0)}$ being as large as possible,
but it won't exceed $1$.
If we combine Eq.~(\ref{dchi2/chi2 0}) and Eq.~(\ref{dchi2/chi2 upper bound}),
and choose $\delta \eta$ such that the least upper bound is equal to 1,
to make sure the process would not going too fast.
Thus we have
\begin{equation}
\eta_1 = \frac{\tau}{\max(\Nbar_f)} = \frac{\min(N_f)}{\max(N_f) - \min(N_f)}.
\end{equation}
Here $N_f$ and $\Nbar_f$ are the eigenvalues of $N$ and $\Nbar$ in the frequency
domain.
If the condition number of noise covariance matrix
$\kappa(N) = \max(N_f)/\min(N_f) \gg 1$,
then $\eta_1 \approx \inv{\kappa} (N)$.

%and in second line we used the property that the positive semi-definite $\Nbar$ is diagonal in frequency 
%domain and its maximum eigenvalue is $\max(\Nbar_f)$.
%To prove this, notice that matrix $\Nbar$ is diagonalized in frequency space 
%with eigenvalues $\Nbar_f\geq0$ and the corresponding eigenvectors $\vb{e}_f$
%(these eigenvectors form a complete orthogonal basis).
%Any vector could be decomposed into these frequency basis
%$\vb{v} = \sum_f \alpha_f \vb{e}_f$, therefore we have
%$\frac{\vb{v}^{\dagger} \Nbar \vb{v}}{\vb{v}^{\dagger} \vb{v}} 
%= \frac{\sum_f \alpha_f^2\Nbar_f}{\sum_f \alpha_f^2}
%\leq \max(\Nbar_f) $
%
%Ideally, we want
%$\delta \chi^2(\hatm(0),0) = \chi^2(\hatm(1), 1) - \chi^2(\hatm(0),0)$,
%such that it would get close to the final $\chi^2$ at next iteration.
%Here if we assume that initial $\chi^2$ value $\chi^2(\hatm(0),0)$ is much
%larger than final value $\chi^2(\hatm(1),1)$,
%then we would expect
%$\qty|\delta\chi^2(\hatm(0),0)/\chi^2(\hatm(0),0)| \approx 1^-$.
%To make sure it will not start too fast, we do not want 
%$\qty|\delta\chi^2(\hatm(0),0)/\chi^2(\hatm(0),0)|$ 
%to exceed $1$.
%So we could set an upper bound $\delta \eta \max(\Nbar_f) / \tau = 1$
%and set
%\begin{equation}
%\eta_1 = \frac{\tau}{\max(\Nbar_f)} = \frac{\min(N_f)}{\max(N_f) - \min(N_f)}
%\end{equation}
%Here $N_f$ is the eigenvalues of noise covariance matrix $N$ under frequency
%domain.
%If the condition number of noise covariance matrix
%$\kappa(N) = \max(N_f)/\min(N_f) \gg 1$,
%then $\eta_1 \approx \inv{\kappa} (N)$.

What about the other parameters $\eta_m$ with $m > 1$?
We use a similar analysis,
letting $\eta_{m+1} = \eta_m + \delta \eta_m$ with a small $\delta\eta_m \ll 1$.
First, let us find the least upper bound
\begin{align}
-\frac{\delta \chi^2(\hatm(\eta_m), \eta_m)}{\chi^2(\hatm(\eta_m), \eta_m)}  
=& \delta\eta_m
\frac{\qty(\vbd - P\hatm(\eta_m))^{\dagger}
    \inv{N_{\eta_m}} \Nbar \inv{N_{\eta_m}}
    (\vbd - P\hatm(\eta_m))
}
{\qty\big(\vbd - P \hatm(\eta_m))^{\dagger}
    \inv{N_{\eta_m}}
    \qty\big(\vbd - P\hatm(\eta_m))
}
\label{dchi2 chi2}
\\
\leq & \delta \eta_m\, \max\qty(\frac{\Nbar_f}{\tau + \eta_m \Nbar_f})
\end{align}
The upper bound in the second line is a little bit tricky.
Both matrix $\Nbar$ and $\inv{N}_{\eta_m}$ 
can be simultaneously diagonalized in frequency space.
For each eigenvector $\vb{e}_f$,
the corresponding eigenvalue of the matrix on the numerator
$\inv{N}_{\eta_m} \Nbar \inv{N}_{\eta_m}$
is
$\lambda_f = \Nbar_f (\tau + \eta_m \Nbar_f)^{-2}$,
and the eigenvalue for matrix on the denominator
$\inv{N}_{\eta_m}$
is
$\gamma_f = (\tau + \eta_m \Nbar_f)^{-1}$.
Their eigenvalues are related by
$\lambda_f = [{\Nbar_f}/{(\tau + \eta_m \Nbar_f)}] \gamma_f$.
For any vector $\vb{v} = \sum_f \alpha_f \vb{e}_f$, we have
\begin{equation}
  \frac{\vb{v}^{\dagger} \inv{N}_{\eta_m} \Nbar \inv{N}_{\eta_m} \vb{v}}
{\vb{v}^{\dagger} \inv{N}_{\eta_m} \vb{v}}
= \frac{\sum_f \alpha_f^2 \lambda_f}{\sum_f \alpha_f^2 \gamma_f}
= \frac{\sum_f \alpha_f^2 \gamma_f \Nbar_f/(\tau + \eta_m \Nbar_f)}
{\sum_f \alpha_f^2 \gamma_f}
\leq \max \qty( \frac{\Nbar_f}{\tau + \eta_m \Nbar_f}).
\end{equation}

Again assuming $\chi^2(\hatm(\eta_{m+1}), \eta_{m+1}) \ll \chi^2(\hatm(\eta_m), \eta_m)$,
which we expect it to be satisfied for $ \eta_m \ll 1$.
That is because if $\eta \lesssim 1$, $\chi^2(\hatm(\eta), \eta)$ would close to the minimum $\chi^2$
which means $\chi^2(\hatm(\eta_{m+1}), \eta_{m+1}) \lesssim \chi^2(\hatm(\eta_m), \eta_m)$,
which would violate our assumption.
Luckily, the final result (\ref{etai rule appendix}) is a geometric series,
only the last few $\eta_m$ values fail to satisfy this condition.
Similarly, we could set the least upper bound equal to 1.
Then we get
\begin{align}
\delta \eta_m 
= \min \qty(\frac{\tau + \eta_m \Nbar_f}{\Nbar_f})
= \eta_m + \frac{\tau }{\max(\Nbar_f)}.
\end{align}
Therefore 
\begin{align}
\eta_{m+1} = \eta_m + \delta\eta_m = 2\eta_m + \frac{\tau }{\max (\Nbar_f)}
\end{align}
The final term ${\tau }/{\max (\Nbar_f)} = \eta_1$ becomes subdominant after a few terms, and we see that the $\eta_m$ increase like a geometric series. 
If written in the form $\eta_{m+1} + {\tau }/{\max(\Nbar_f)}
= 2 \qty( \eta_m + {\tau}/{\max(\Nbar_f)})$
it's easy to see that for $m \geq 1$,
$\eta_{m} + {\tau }/{\max(\Nbar_f)}$ forms a geometric series
\begin{align}
\eta_m +  \frac{\tau }{\max(\Nbar_f)}
=\qty(\eta_1 + \frac{\tau }{\max(\Nbar_f)}) 2^{m-1}
=\frac{\tau}{\max(\Nbar_f)} 2^m
\end{align}
where we used $\eta_1 = {\tau}/{\max(\bar{N}_f)}$.
Note that $m = 0$ and $\eta_0 = 0$ also satisfy this expression and we've got
final expression for all $\eta_m$
\begin{align}
\eta_m =\min \qty\bigg{1,\; \frac{\tau}{\max(\Nbar_f)} \qty(2^m -1) }
\label{etai rule}
\end{align}
Here we need to truncate the series when $\eta_m > 1$.

%What about the other parameters $\eta_m$ with $m > 1$?
%We could use a similar analysis,
%and suppose $\eta_{m+1} = \eta_m + \delta \eta_m$ with a small $\delta\eta_m$,
%and find the fractional decrease
%\begin{align}
%\begin{aligned}[b]
%-\frac{\delta \chi^2(\hatm(\eta_m), \eta_m)}{\chi^2(\hatm(\eta_m), \eta_m)}  
%&= \delta\eta_m
%\frac{\qty(\vbd - P\hatm(\eta_m))^{\dagger}
%    \inv{N_{\eta_m}} \Nbar \inv{N_{\eta_m}}
%    (\vbd - P\hatm(\eta_m))
%}
%{\qty\big(\vbd - P \hatm(\eta_m))^{\dagger}
%    \inv{N_{\eta_m}}
%    \qty\big(\vbd - P\hatm(\eta_m))
%}
%\\
%& \leq \delta \eta_m\, \max\qty(\frac{\Nbar_f}{\tau + \eta_m \Nbar_f})
%\label{eta upper bound}
%\end{aligned}
%\end{align}
%The least upper bound in the second line is a little bit tricky.
%Both matrix $\Nbar$ and $\inv{N}_{\eta_m}$ 
%can be simultaneously diagonalized in frequency space.
%For each eigenvector $\vb{e}_f$,
%the corresponding eigenvalues of the matrix 
%$\inv{N}_{\eta_m} \Nbar \inv{N}_{\eta_m}$
%are
%$\lambda_f = \Nbar_f (\tau + \eta_m \Nbar_f)^{-2}$,
%and the eigenvalues for matrix 
%$\inv{N}_{\eta_m}$
%are
%$\gamma_f = (\tau + \eta_m \Nbar_f)^{-1}$.
%Their eigenvalues are related by
%$\lambda_f = \frac{\Nbar_f}{\tau + \eta_m \Nbar_f} \gamma_f$.
%For any vector $\vb{v} = \sum_f \alpha_f \vb{e}_f$, we have
%$\frac{\vb{v}^{\dagger} \inv{N}_{\eta_m} \Nbar \inv{N}_{\eta_m} \vb{v}}
%{\vb{v}^{\dagger} \inv{N}_{\eta_m} \vb{v}}
%= \frac{\sum_f \alpha_f^2 \lambda_f}{\sum_f \alpha_f^2 \gamma_f}
%= \frac{\sum_f \alpha_f^2 \gamma_f \Nbar_f/(\tau + \eta_m \Nbar_f)}
%{\sum_f \alpha_f^2 \gamma_f}
%\leq \max \qty( \frac{\Nbar_f}{\tau + \eta_m \Nbar_f})
%$
%
%Similarly, we could set the least upper bound
%$\delta\eta_m \max \qty( \frac{\Nbar_f}{\tau + \eta_m \Nbar_f})=1$,
%\footnote{Here we also assumed that
%$\chi^2(\hatm(\eta_m),\eta_m) \gg \chi^2(\hatm(1),1)$,
%which we expect it to be satisfied for $0 \simeq \eta_m \ll 1$. 
%Since final result Eq.(\ref{etai rule}) is geometric series,
%only a few $\eta_m$ values won't satisfy this condition.
%}
%and then we get
%\begin{align}
%\delta \eta_m 
%= \min \qty(\frac{\tau + \eta_m \Nbar_f}{\Nbar_f})
%= \eta_m + \frac{\tau }{\max(\Nbar_f)}.
%\end{align}
%Therefore 
%\begin{align}
%\eta_{m+1} = \eta_m + \delta\eta_m = 2\eta_m + \frac{\tau }{\max (\Nbar_f)}
%\end{align}
%As we can see, $\eta_1, \cdots, \eta_n$ should increase like a geometric
%series. 
%And written in the form $\eta_{m+1} + \frac{\tau }{\max(\Nbar_f)}
%= 2 \qty( \eta_m + \frac{\tau}{\max(\Nbar_f)})$
%it's easy to see that for $m \geq 1$,
%$\eta_{m} + \frac{\tau }{\max(\Nbar_f)}$ forms a geometric series
%\begin{align}
%\eta_m +  \frac{\tau }{\max(\Nbar_f)}
%=\qty(\eta_1 + \frac{\tau }{\max(\Nbar_f)}) 2^{m-1}
%=\frac{\tau}{\max(\Nbar_f)} 2^m
%\end{align}
%Note that $m = 0$ and $\eta_0 = 0$ also satisfy this expression and we've got
%final expression for all $\eta_i$
%\begin{align}
%\eta_i =\min \qty\bigg{1,\; \frac{\tau}{\max(\Nbar_f)} \qty(2^i -1) }
%\label{etai rule}
%\end{align}
%Here we need to truncate the series when $\eta_i > 1$.

This is the main result.  Eq.~(\ref{etai rule}) tells us not only how to choose parameters $\eta_i$,
but also when we should stop the perturbation, and set $\eta = 1$.
For example, if noise covariance matrix $N$ is almost white noise,
then $\Nbar = N - \tau I \approx 0$,
and we would have $\frac{\tau}{\max(\Nbar_f)} \gg 1$.
This tell us that we don't need to use parameterized method at all, 
because $\eta_1 = 1$.
Note that the vanilla conjugate gradient method with simple binned map as
initial guess corresponds to choosing $\eta_0=0$ and $\eta_1= \eta_2 = \cdots
= 1$.


\subsubsection{Intuitive Interpretation of $\eta$}\label{intuitive interp}

In this section, let me introduce another way to understand the role of $\eta$.
Our ultimate goal is to find $\hatm(1)$ which minimizes $\chi^2(\vbm)$ in Eq.~(\ref{chi2 formula}).
Since $N$ is diagonal in frequency space,
$\chi^2$ could be written as a sum of all frequency mode
$\qty|(\vbd-P\vbm)_f|^2$ with weight $\inv{N}_f$, such as
$\chi^2(\vbm) = \sum_f \qty|(\vbd-P\vbm)_f|^2 \inv{N}_f$.
The weight is large for low noise frequency mode (small $N_f$), and vice versa.
Which means $\chi^2(\vbm)$ would favor the low noise frequency mode over high
noise ones.
In other words the optimal map $\hatm$ focusing on minimize the error
$\vb*{\varepsilon} \equiv \vbd - P\vbm$ in the low-noise part.

After introducing $\eta$, we minimize
$\chi^2(\vbm,\eta)$ in Eq.~(\ref{chi2 eta formula}) instead.
For $\eta=0$, $N^{-1}(0) \propto I$ the system is homoscedastic and the estimated map $\hatm(0)$
does not prioritize any frequency mode.
As we slowly increase $\eta$, we decrease the weight for the high noise modes,
and focusing minimizing error for low noise part.
If we start with $\eta_1=1$ directly, which corresponds to the vanilla conjugate
gradient method, then the entire conjugate gradient solver
will focus most on minimizing the low noise part, such that $\chi^2$ would
converge very fast at low noise region, but slowly on high noise part.
It may be stuck at some local minimum point and hard to get to global minimum.
However by introducing $\eta$ parameter, we let the solver first treat every
frequency equally,
then as $\eta$ slowly increases, it gradually give more focus to the lowest noise part.

%Our ultimate goal is to find $\hatm(\eta=1)$ which minimizes 
%$\chi^2(\vbm) = (\vbd - P\vbm)^{\dagger} \inv{N} (\vbd - P\vbm)$.
%Here we also assumed that $N$ is diagonal in frequency space.
%With this condition $\chi^2$ could be written as a sum of all frequency mode 
%$\qty|(\vbd-P\vbm)_f|^2$ with weight $\inv{N}_f$, such as
%$\chi^2(\vbm) = \sum_f \qty|(\vbd-P\vbm)_f|^2 \inv{N}_f$.
%$\inv{N}_f$ is large when there is little noise at that frequency,
%and vice versa.
%Which means $\chi^2(\vbm)$ would favor the low noise frequency mode over high 
%noise ones, because low noise part has higher weight.
%In other words the optimal map $\hatm$ focusing on minimize the error
%$\vb{r} \equiv \vbd - P\vbm$ in the low-noise part.
%
%After introducing $\eta$, we minimize
%$\chi^2(\vbm,\eta)=(\vbd-P\vbm)^{\dagger} N_{\eta}^{-1} (\vbd - P\vbm)$
%for each $\eta$ value as it increase from $0$ to $1$.
%For $\eta=0$, $N^{-1}_{\eta=0} \propto I$ and the estimated map $\hatm(\eta=0)$
%does not prioritize any frequency mode when minimizing the error.
%As we slowly increase $\eta$, we decrease the weight for the frequency modes
%which have large noise, and focusing minimizing error for low noise part.
%If we start with $\eta_1=1$ directly, which corresponds to the vanilla conjugate
%gradient method, then the entire conjugate gradient solver
%will only focusing on minimizing low noise part, such that $\chi^2$ would
%converge very fast at low noise region, but relative slow on high noise part.
%However by introducing $\eta$ parameter, we let the solver first treat every
%frequency equally.
%Then as $\eta$ slowly increases, it gradually shifts focus to low noise
%part.
%If we write the difference between final and initial $\chi^2$ value as
%$\chi^2(\hatm(1),1) - \chi^2(\hatm(0),0) = \int_0^1 \dd{\eta}
%\dv{\eta} \chi^2(\hatm(\eta),\eta)$,
%and use Eq.~(\ref{d chi2})
%\begin{align}
%\dv{\eta} \chi^2(\hatm(\eta), \eta) 
%&= - \qty(\vbd - P\hatm(\eta))^{\dagger} \inv{\Neta} \Nbar \inv{\Neta} 
%    (\vbd - P\hatm(\eta)) \tag{\ref{d chi2}}
%\end{align}
%we note that when $\eta$ is very small, 
%the $\dv{\eta}\chi^2(\hatm(\eta),\eta)$ would have relatively large
%contribution from medium to large noise region, comparing to large $\eta$.
%So introducing $\eta$ might improve the convergence of $\chi^2$ at these
%regions, because the vanilla conjugate gradient method only focuses on the low noise
%part and it may have difficulty at these regions.
%
%
%\subsection{Computational Cost}
%To properly compare the performance cost of this method with respect to vanilla
%conjugate gradient method with simple preconditioner,
%we need to compare their computational cost at each iteration.
%The right hand side of parameterized map making equation
%Eq.(\ref{map making para})
%\begin{align}
%\qty(\Pdagger \inv{N(\eta)} P)\, \hatm(\eta) 
%= \Pdagger \inv{N(\eta)} \vbd \tag{\ref{map making para}}
%\end{align}
%could be computed before iterations,
%so it won't introduce extra computational cost during iterations.
%The most demanding part of conjugate gradient method is calculating
%$\Pdagger \inv{N} P \hatm$, because it contains a Fourier transform of
%$P\hatm$ from time domain to frequency domain and an inverse Fourier transform
%of $\inv{N} P \hatm$ from frequency domain back to time domain,
%which is order $\mathcal{O}(n\log n)$ with $n$ being the length of time ordered
%data.
%If we change $\inv{N}$ to $\inv{N(\eta)}$, it won't add extra cost,
%since both matrices are diagnal in frequency domain.
%Therefore the computational cost it the same for one step.
%
%However in previous analysis, our choice of parameters $\eta_i$ is based on
%$\delta\chi^2(\hatm(\eta_i), \eta_i)$ which is evaluated at 
%$\vbm = \hatm(\eta_i)$ the estimated map at $\eta_i$.
%We update $\eta_i$ to $\eta_{i+1}$ when $\vbm \approx \hatm(\eta_i)$. 
%How do we know current map $\vbm$ is close to $\hatm(\eta_i)$? 
%Because for each new $\eta_i$ value, we are solving a new set of linear
%equations $A(\eta_i) \hatm = \vbb(\eta_i)$ with
%$A(\eta_i) = \Pdagger \inv{N(\eta_i)} P$ and 
%$\vbb(\eta_i) = \Pdagger \inv{N(\eta_i)} \vbd$,
%and we could stop calculation and moving to next value $\eta_{i+1}$ when the 
%norm of residual 
%$\qty||\vb{r}(\eta_i)|| = \qty||\vbb(\eta_i) - A(\eta_i) \vbm||$
%smaller than some specific value.
%Since when doing conjugate gradient algorithm we calculate $\vb{r}$ and stop
%the iteration when $\qty||\vb{r}||$ is small enough, now after introducing
%parameter $\eta$, we move to next parameter $\eta_{i+1}$ when 
%$\qty||\vb{r}(\eta_i)||$ is small enough.
%Again, this won't add extra cost compare to vanilla conjugate gradient method.
%
%Therefore we find that the only significant cost after adding perturbation
%parameter $\eta$, is to to find out $\vb{b}(\eta_i)$ for each $\eta_i\neq1$
%before starting the iterations.
%And this is one time calculation, it's negligible compare to remaining
%calculations.




\section{Messenger Field Method}
The messenger field method is a fixed point iterative solver introduced by
Elsner and Wandelt (2013) \cite{2013A&A...549A.111E} to solve Wiener filter.
Later on Huffenberger and N{\ae}ss (2018) \cite{Huffenberger_2018} applied this
method to map-making problem, and showed that in some cases messenger field is
better than conjugate gradient with a simple preconditioner.
 Pape\v{z} el al.(2018) \cite{2018A&A...620A..59P} proved that messenger
field is equivalent to apply a preconditioner to map making equation
Eq.(\ref{map making equation}), and it can be solved using both fixed point
iteration and preconditioned conjugate gradient methods.
They showed that in some conjugate gradient with simple preconditioner
outperforms messenger field method with both fixed point iteration and
preconditioned conjugate gradient methods.

Messenger field method similarly separate noise covariance matrix
$N = \Nbar + T$, with $T = \tau I $ and $\tau$ being the minimum eigenvalue of
$N$.
Then there is a cooling parameter $\lambda$ such that 
$N(\lambda) = \Nbar + \lambda T$, with initial $\lambda$ being a very large
number and final $\lambda$ being $1$.
As you might guess $\lambda$ is related to $\eta$ by $\lambda = 1/\eta$.

Before introducing messenger field method, let's first prove one identity
\begin{align}
\begin{aligned}[b]
&\mathrel{\phantom{=}}
\PPinv{\inv{T}} \Pdagger \inv{T} \inv{\qty(\inv{T} + \inv{\Nbar})} \inv{T} P
\\
&= \PPinv{\inv{T}} \Pdagger \inv{T} \inv{\qty(I + T\inv{\Nbar})}  P
\\
&= \PPinv{\inv{T}} \Pdagger \inv{T}
    \qty(I - T\inv{\Nbar}+ T\inv{\Nbar}T\inv{\Nbar} - \cdots) P
\\
&= I - \PPinv{\inv{T}} \Pdagger \cancel{\inv{T}T} \inv{\Nbar} 
    \qty(I - T\inv{\Nbar} + T\inv{\Nbar}T\inv{\Nbar} - \cdots)P
\\
&= I - \PPinv{\inv{T}} \Pdagger \inv{\Nbar} \inv{\qty(I + T\inv{\Nbar})} P
\\
&= I - \PPinv{\inv{T}} \Pdagger \inv{\qty(\Nbar + T)} P
\\
&= I - \PPinv{\inv{T}} \Pdagger \inv{N} P \label{MF identity}
\end{aligned}
\end{align}
where at third and fifth line we used expansion
$\inv{\qty(I + A)} = I - A + A^2 - \cdots $

After apply preconditioner $\Pdagger \inv{T} P$ to the map making equation
Eq.(\ref{map making equation}), we get:
\begin{align}
\begin{aligned}[b]
\mathrel{\phantom{\Rightarrow}}
&\PPinv{\inv{T}} \qty(\Pdagger \inv{N} P) \hatm
= \PPinv{\inv{T}} \Pdagger \inv{N} \vbd
\\ \Rightarrow
&\hatm - \PPinv{\inv{T}} \Pdagger \inv{T} 
    \inv{\qty(\inv{T}+\inv{\Nbar})} \inv{T} P\hatm
= \PPinv{\inv{T}} \Pdagger \inv{N} \vbd
\\ \Rightarrow
& \hatm = \PPinv{\inv{T}} \Pdagger \inv{T} \inv{\qty(\inv{T} + \inv{\Nbar})} 
    \qty[ \inv{T}P\hatm + \qty(\inv{T}+\inv{\Nbar}) T \inv{N}\vbd]
\\ \Rightarrow
& \hatm = \PPinv{\inv{T}} \Pdagger \inv{T} \inv{\qty(\inv{T} + \inv{\Nbar})} 
    \qty[ \inv{T}P\hatm + \qty(I+\inv{\Nbar}T) \inv{(\Nbar + T)}\vbd]
\\ \Rightarrow
& \hatm = \PPinv{\inv{T}} \Pdagger \inv{T} 
    \inv{\qty(\inv{T} + \inv{\Nbar})} \qty[ \inv{T}P\hatm + \inv{\Nbar}\vbd]
\end{aligned}
\end{align}
where the second line we used identity Eq.(\ref{MF identity}).

To add cooling parameter $\lambda$, we only need to change $T$ to $\lambda T$
and $N$ to $N(\lambda)$.
Then we could  write it as fixed point iteration form
\begin{equation}
\left\{\!
\begin{aligned}
\vb{t}_i &= \inv{\qty(\inv{(\lambda T)} + \inv{\Nbar})} 
    \qty[ \inv{(\lambda T)}P\hatm_i + \inv{\Nbar}\vbd]\\
\hatm_{i+1} &= \PPinv{\inv{(\lambda T)}} \Pdagger \inv{(\lambda T)} \vb{t}_i 
\end{aligned}
\right.
\end{equation}
This is fixed point iteration form of messenger field method.
It's solving map making equation Eq.(\ref{map making equation}) with
preconditioner $\Pdagger \inv{(\lambda T)} P$
\begin{align}
\PPinv{\inv{(\lambda T)}} \Pdagger \inv{(\Nbar + \lambda T)} P \hatm
= \PPinv{\inv{(\lambda T)}} \Pdagger \inv{(\Nbar + \lambda T)} \vbd
\end{align}
substitute $T = \tau I$ 
\begin{align}
\tau \PPinv{} \Pdagger \inv{\qty(\tau I + \frac{1}{\lambda}\Nbar)} P \hatm 
= \tau \PPinv{} \Pdagger \inv{\qty(\tau I + \frac{1}{\lambda}\Nbar)} \vbd
\label{lambda eta equiv}
\end{align}
since multiplying a constant won't change the condition number, it's equivalent
to solve map making equation with perturbation parameter $\eta = 1/\lambda$ and
simple preconditioner.

%\section{Numerical Simulations}
%To compare these algorithms, we need to do some simple simulation of scanning
%processes, and generate time ordered data from random sky signal.
%Our sky is a small rectangular area, with two orthogonal directions $x$ and
%$y$, both with range from $-1\degree$ to $+1\degree$.
%The electromagnetic signal is described as four stokes parameters
%$(S_0, S_1, S_2, S_3) = (I,Q,U,V)$.
%We model the overall electromagnetic signal is created by some normal
%distributed sources in the sky, with intensity $I_i (x,y)
%= A_i \exp\qty(-\frac{1}{2} \frac{(x-x_{i})^2 + (y-y_{i})^2}{\sigma_i^2})$,
%for each source centered at $(x_i,y_i)$.
%In our simulation, $A_i \sim \text{Unif} (-100, 100)$,
%$\sigma_i \sim \text{Unif}(0.05\degree, 0.2\degree)$ 
%and the center of each source
%$x_i, y_i \sim \text{Unif}(-1\degree, +1\degree)$.
%Every source has its degree of polarization $p_i \sim \text{Unif}(0,1)$ and 
%polarization angle $\psi_i \sim \text{Unif}(0,\pi)$.
%Here we ignored angle $\chi_i$, because our detectors won't be sensitive to
%circular polarization.
%Finally, the stokes parameters over sky is given by
%$S_0(x,y) = \sum_i I_i(x,y)$, $S_1(x,y) = \sum_i I_i(x,y) p_i \cos(2\psi_i)$,
%$S_2(x,y) = \sum_i I_i(x,y) p_i \sin(2\psi_i)$.
%Again, we ignored $S_3$, because it describes circular polarization.
%
%For the scanning process, our single telescope contains nine detectors,
%each has different sensitivity to polarization $S_1$ and $S_2$.
%It scans the sky with a raster scanning pattern and scanning frequency
%$f_{\text{scan}} = 0.1$ Hz sampling frequency $f_{\text{sample}} = 100$ Hz.
%The telescope scans the sky horizontally and then vertically,
%and then digitizes the position $(x, y)$ into $512\times 512$ pixel.
%This gives noiseless signal $\vb{s}$.
%
%The noise power spectrum is given by
%\begin{align}
%P(f) = \sigma^2 \qty(1+ \frac{f_{\text{knee}}^{\alpha}+f_{\text{apo}}^{\alpha}}
%    {f^{\alpha}+f_{\text{apo}}^{\alpha}}) \label{noise power spectrum}
%\end{align}
%Here we fixed $\sigma^2 = 10$ $\mu$K$^2$, $\alpha = 2$ and $f_{\text{knee}} = 10$ Hz,
%and change $f_{\text{apo}}$ to compare the performance under different noise
%models.
%Note that as $f_{\text{apo}} \rightarrow 0 $,
%$P(f) \rightarrow \sigma^2\qty(1 + (f/f_{\text{knee}})^{-1} )$, 
%it becomes a $1/f$ noise model.
%The noise covariance matrix 
%\begin{equation}
%N_{ff'} = P(f) \frac{\delta_{ff'}}{\Delta_f}
%\end{equation}
%is a diagonal matrix in frequency space, where $\Delta_f$ is equal to reciprocal
%of total scanning time $T$.
%
%Finally, we get the simulated time ordered data $\vb{d} = \vb{s} + \vb{n}$ by
%adding up signal and noise.
%
%\section{Results}
%First let's compare the results with vanilla conjugate gradient method with
%simple preconditioner $\Pdagger P$.
%The results are showed in Figure (\ref{small condi num CG}) for different kinds
%of noise power spectra.
%Here note that $\chi^2$ in Figure (\ref{small condi num chi2 CG}) is calculated
%based on Eq.(\ref{chi2 formula})
%\begin{align}
%\chi^2(\vbm) = (\vbd - P\vbm)^{\dagger} N^{-1} (\vbd - P \vbm)
%\tag{\ref{chi2 formula}}
%\end{align}
%not $\chi^2(\vbm, \eta)$ in Eq.(\ref{chi2 eta formula}).
%The $\chi^2_{\text{min}}$ is calculated from perturbative conjugate gradient
%method with more intermediate $\eta$ values, and more iterations after
%$\eta=1$.
%
%As we can see in Figure(\ref{small condi num chi2 CG}), if the condition number
%of noise covariance matrix $\kappa(N)$ is small, and the noise is almost white
%noise, the performance between different these two methods is small.  The vanilla conjugate gradient method converge faster, because its perturbation
%parameter $\eta_{i}=\{0,1,1,\cdots\}$, however for the perturbation method its
%$\eta$ value will slowly reach $1$ in first few iterations as we can see in
%Figure(\ref{small condi num eta CG}).
%
%\begin{figure}[htb]
%\centering
%\begin{subfigure}{0.33\textwidth}
%    \centering
%    \includegraphics[width=\linewidth]
%        {./images/0.1/small_condition_num/P_f.pdf}
%    \caption{}
%    \label{small condi num power spectrum CG}
%\end{subfigure}%
%\begin{subfigure}{0.33\textwidth}
%    \centering
%    \includegraphics[width=\linewidth]
%        {./images/0.1/small_condition_num/chi2_CG.pdf}
%    \caption{}
%    \label{small condi num chi2 CG}
%\end{subfigure}%
%\begin{subfigure}{0.33\textwidth}
%    \centering
%    \includegraphics[width=\linewidth]
%        {./images/0.1/small_condition_num/eta_CG.pdf}
%    \caption{}
%    \label{small condi num eta CG}
%\end{subfigure}
%\caption{The left graph shows the noise power spectrum
%    Eq.(\ref{noise power spectrum}) with $f_{\text{apo}} \approx 0.99$ and
%    $\kappa(N) = 10^2$. The center one shows the
%    $\chi^2(\vbm)/\chi^2_{\text{final}} - 1$, with $\chi^2(\vbm)$ calculated
%    based on Eq.(\ref{chi2 formula}).
%    The right one shows the $\eta$ value for each iteration. For vanilla 
%    conjugate gradient method $\eta$ always equal to $1$, so it's a horizontal
%    line at $\eta=1$.
%}
%\label{small condi num CG}
%\end{figure}
%
%Notice that as we increase $\kappa(N)$, or equivalently decrease
%$f_{\text{apo}}$, the perturbation parameter $\eta$ starts showing its 
%benefits, as showed in Figure(\ref{medium condi num CG}) and 
%Figure(\ref{large condi num CG}).
%It outperforms the vanilla conjugate gradient method when 
%$f_{\text{apo}} \approx 0$ and the noise power spectrum becomes the $1/f$ noise model,
%which usually is the intrinsic noise of instruments (\cite{1997PhRvD..56.4514T}).
%\begin{figure}[htb]
%\centering
%\begin{subfigure}{0.33\textwidth}
%    \centering
%    \includegraphics[width=\linewidth]
%        {./images/0.1/medium_condition_num/P_f.pdf}
%    \caption{}
%    \label{medium condi num power spectrum CG}
%\end{subfigure}%
%\begin{subfigure}{0.33\textwidth}
%    \centering
%    \includegraphics[width=\linewidth]
%        {./images/0.1/medium_condition_num/chi2_CG.pdf}
%    \caption{}
%    \label{medium condi num chi2 CG}
%\end{subfigure}%
%\begin{subfigure}{0.33\textwidth}
%    \centering
%    \includegraphics[width=\linewidth]
%        {./images/0.1/medium_condition_num/eta_CG.pdf}
%    \caption{}
%    \label{medium condi num eta CG}
%\end{subfigure}
%\caption{The figure shows results for $f_{\text{apo}}\approx 9.8\times10^{-3}$ 
%    and $\kappa(N) = 10^6$.
%}
%\label{medium condi num CG}
%\end{figure}
%
%\begin{figure}[htb]
%\centering
%\begin{subfigure}{0.33\textwidth}
%    \centering
%    \includegraphics[width=\linewidth]
%        {./images/0.1/large_condition_num/P_f.pdf}
%    \caption{}
%    \label{large condi num power spectrum CG}
%\end{subfigure}%
%\begin{subfigure}{0.33\textwidth}
%    \centering
%    \includegraphics[width=\linewidth]
%        {./images/0.1/large_condition_num/chi2_CG.pdf}
%    \caption{}
%    \label{large condi num chi2 CG}
%\end{subfigure}%
%\begin{subfigure}{0.33\textwidth}
%    \centering
%    \includegraphics[width=\linewidth]
%        {./images/0.1/large_condition_num/eta_CG.pdf}
%    \caption{}
%    \label{large condi num eta CG}
%\end{subfigure}
%\caption{The figure shows results for $f_{\text{apo}}\approx 9.8\times10^{-6}$ 
%    and $\kappa(N) = 10^{12}$.
%}
%\label{large condi num CG}
%\end{figure}
%
%In the conjugate gradient method with messenger cooling parameter $\lambda$, 
%the number of cooling parameters we need is an extra free parameter.
%After the number of $\lambda$ is determined, we construct a geometric series
%with fixed initial and final value, which uses \texttt{logspace}
%function in \texttt{numpy}.
%Since I  show in Eq.(\ref{lambda eta equiv}) that the messenger field
%cooling parameter $\lambda$ is equivalent to $1/\eta$, I use $\eta$ for further analysis.
%
%Now let us compare the performance difference between choosing $\eta$
%parameters based on Eq.(\ref{etai rule})
%and fixing number of $\eta$ parameters $n_{\eta}$ manually.
%Here we choose the $\eta_i$ values using function
%\texttt{numpy.logspace(start=$\ln(\eta_1)$, stop=0, num=$n_{\eta}$, base=$e$)}.
%The results are showed in Figure(\ref{small condi num}),
%(\ref{medium condi num}), and (\ref{large condi num}).
%
%When $\kappa(N)$ is small, and Eq.(\ref{etai rule}) tells us that only a few
%$\eta$ parameters are good enough, see Figure(\ref{small condi num chi2}).
%If unfortunately we choose $n_{\eta}$ being large value, like $15$ or $30$,
%then it will ends up converge slowly, because it needs at least $15$ or $30$
%iterations to reach $\eta=1$, at least one iteration per $\eta$ level.
%
%On the other hand if $\kappa(N)$ is very large and the power spectrum is $1/f$
%noise, we need more $\eta$ parameters.
%If $n_{\eta}$ is too small, for example $n_{\eta}=5$ in
%Figure(\ref{large condi num chi2}), it may be better than the vanilla conjugate
%gradient method, but it is still far from optimal.
%
%
%\begin{figure}[htb]
%\centering
%\begin{subfigure}{0.33\textwidth}
%    \centering
%    \includegraphics[width=\linewidth]
%        {./images/0.1/small_condition_num/P_f.pdf}
%    \caption{}
%    \label{small condi num power spectrum}
%\end{subfigure}%
%\begin{subfigure}{0.33\textwidth}
%    \centering
%    \includegraphics[width=\linewidth]
%        {./images/0.1/small_condition_num/chi2.pdf}
%    \caption{}
%    \label{small condi num chi2}
%\end{subfigure}%
%\begin{subfigure}{0.33\textwidth}
%    \centering
%    \includegraphics[width=\linewidth]
%        {./images/0.1/small_condition_num/eta.pdf}
%    \caption{}
%    \label{small condi num eta}
%\end{subfigure}
%\caption{Same as Figure(\ref{small condi num CG}) with extra manually chosen 
%    $n_{\eta}$ results.
%}
%\label{small condi num}
%\end{figure}
%
%
%\begin{figure}[htb]
%\centering
%\begin{subfigure}{0.33\textwidth}
%    \centering
%    \includegraphics[width=\linewidth]
%        {./images/0.1/medium_condition_num/P_f.pdf}
%    \caption{}
%    \label{medium condi num power spectrum}
%\end{subfigure}%
%\begin{subfigure}{0.33\textwidth}
%    \centering
%    \includegraphics[width=\linewidth]
%        {./images/0.1/medium_condition_num/chi2.pdf}
%    \caption{}
%    \label{medium condi num chi2}
%\end{subfigure}%
%\begin{subfigure}{0.33\textwidth}
%    \centering
%    \includegraphics[width=\linewidth]
%        {./images/0.1/medium_condition_num/eta.pdf}
%    \caption{}
%    \label{medium condi num eta}
%\end{subfigure}
%\caption{Same as Figure(\ref{medium condi num CG}) with extra manually chosen 
%    $n_{\eta}$ results.
%}
%\label{medium condi num}
%\end{figure}
%
%\begin{figure}[htb]
%\centering
%\begin{subfigure}{0.33\textwidth}
%    \centering
%    \includegraphics[width=\linewidth]
%        {./images/0.1/large_condition_num/p_f.pdf}
%    \caption{}
%    \label{large condi num power spectrum}
%\end{subfigure}%
%\begin{subfigure}{0.33\textwidth}
%    \centering
%    \includegraphics[width=\linewidth]
%        {./images/0.1/large_condition_num/chi2.pdf}
%    \caption{}
%    \label{large condi num chi2}
%\end{subfigure}%
%\begin{subfigure}{0.33\textwidth}
%    \centering
%    \includegraphics[width=\linewidth]
%        {./images/0.1/large_condition_num/eta.pdf}
%    \caption{}
%    \label{large condi num eta}
%\end{subfigure}
%\caption{Same as Figure(\ref{large condi num CG}) with extra manually chosen 
%    $n_{\eta}$ results.
%}
%\label{large condi num}
%\end{figure}
%
%\section{Possible improvements}
%As you may have noticed in Figure(\ref{medium condi num})
%and Figure(\ref{large condi num}), the perturbation parameter based on
%Eq.(\ref{etai rule}) is more than needed, especially for $1/f$ noise case.
%From Figure(\ref{large condi num eta}) we know that Eq.(\ref{etai rule}) gives
%us $n_{\eta}\approx40$, however based on $\chi^2$ result in 
%Figure(\ref{large condi num chi2}), we notice that $n_{\eta}\approx30$ or 
%even $n_{\eta} \approx 15$ is good enough.
%Also, for the nearly-white-noise case, we could certainly choose $n_{\eta}=1$
%such that $\eta_1=1$ which corresponds to vanilla conjugate gradient method,
%based on $\chi^2$ result in Figure(\ref{small condi num chi2}).
%However Eq.(\ref{etai rule}) gives us $n_{\eta} \approx 6$,
%see Figure(\ref{small condi num eta}), even though it does not make the final 
%$\chi^2$ result much different at the end.
%
%Is it possible to further improve the analysis, such that it produces
%smaller $n_{\eta}$?
%Let's examine how we get $\eta_i$ series.
%Remember that we determine $\delta\eta$ value based on the upper bound of 
%$-\delta\chi^2(\hatm(\eta), \eta)/\chi^2(\hatm(\eta), \eta)$, in
%Eq.(\ref{eta upper bound}).
%Here I rewrite it in a simplified form
%\begin{align}
%-\frac{\delta\chi^2(\hatm(\eta), \eta)}{\chi^2(\hatm(\eta), \eta)}
%= -\delta\eta
%    \frac{\dv{\eta} \chi^2(\hatm(\eta), \eta)}
%    {\chi^2(\hatm(\eta), \eta)}
%= \delta\eta \frac{\hat{\vb{r}}_{\eta}^{\dagger} \inv{N}_{\eta} \Nbar 
%    \inv{N}_{\eta} \hat{\vb{r}}_{\eta} }
%    {\hat{\vb{r}}_{\eta}^{\dagger} \inv{N}_{\eta} \hat{\vb{r}}_{\eta} }
%\leq  \frac{\delta \eta}{\eta + \frac{\tau}{\max(N_f) -\tau}}
%\end{align}
%with
%$\vb{r}_{\eta} = \vbd - P\hatm(\eta) 
%= \qty[ 1 - P\PPinv{\inv{N_{\eta}}}\Pdagger \inv{N_{\eta}}]\vbd
%\equiv \mathcal{P}_{\eta} \vbd$.
%We treated $\vb{r}_{\eta}$ as an arbitrary 
%vector in frequency domain, since we don't know how to calculate 
%$\mathcal{P}_{\eta}$ for $\eta \neq 0$, and it's hard to 
%analyze the projection matrix $\mathcal{P}_{\eta}$ in frequency space,
%as it contains $\PPinv{\inv{N_{\eta}}}$.
%Note that we have to determine all of $\eta$ value before calculation, 
%because we don't want to keep the time ordered data in system RAM,
%so we need to somehow analytically analyze $\mathcal{P}_{\eta}$, and its behavior
%in frequency space.
%
%Unless $\vb{r}_{\eta}$ almost only has large noise modes,
%$\dv{\eta}\chi^2(\hatm(\eta),\eta)/\chi^2(\hatm(\eta),\eta)$
%won't get close to the upper bound
%$1/\qty(\eta + \frac{\tau}{\max(N_f) -\tau})$.
%Based on the analysis in Section(\ref{intuitive interp}),
%for small $\eta$ the estimated map $\hatm(\eta)$ does not only focusing on 
%minimizing error $\vb{r}_{\eta}$ at low noise region.
%So we would expect that there would be a fair amount of low noise modes
%contribution in $\vb{r}_{\eta}$ especially for the first few $\eta$ values.
%Which means if we could somehow know the frequency distribution of 
%$\vb{r}_{\eta}$, we could tighten the boundary of
%$\dv{\eta}\chi^2(\hatm(\eta),\eta)/\chi^2(\hatm(\eta),\eta)$,
%and get larger $\delta\eta$ value.
%This should make $\eta$ goes to $1$ faster, and yields the fewer $\eta$ parameters 
%we need.
%
%Also notice that the $\eta$ values determined from Eq.(\ref{etai rule})
%\begin{align}
%\eta_i =\min \qty\bigg{1,\; \frac{\tau}{\max(\Nbar_f)} \qty(2^i -1) } 
%\tag{\ref{etai rule}}
%\end{align}
%are not dependent on any scanning information,
%it only depends on noise power spectrum $P(f)$, or noise covariance matrix $N$.
%Figure(\ref{large condi num 0.001}) and Figure(\ref{large condi num 10}) show
%two examples with same parameters as in Figure(\ref{large condi num}) except 
%scanning frequency $f_{\text{scan}}$, in Figure(\ref{large condi num 0.001}) it
%scans very slow and in Figure(\ref{large condi num 10}) it's very fast.
%In these two cases our $\eta$ values based on Eq.(\ref{etai rule}) are better
%than manually selected values.
%Based on these two results we know, the $\eta$ values should somehow depends
%on scanning scheme.
%Again that's because when we determine the upper bound of 
%$\dv{\eta} \chi^2(\hatm(\eta), \eta)$ we treat
%$\vb{r}_{\eta} = \vbd - P\hatm = \mathcal{P}_{\eta} \vbd$
%as an arbitrary vector, such that we lose all information related to scanning 
%scheme in the pointing matrix $P$.
%
%
%\begin{figure}[htb]
%\centering
%\begin{subfigure}{0.33\textwidth}
%    \centering
%    \includegraphics[width=\linewidth]
%        {./images/0.001/large_condition_num/p_f.pdf}
%    \caption{}
%    \label{large condi num power spectrum 0.001}
%\end{subfigure}%
%\begin{subfigure}{0.33\textwidth}
%    \centering
%    \includegraphics[width=\linewidth]
%        {./images/0.001/large_condition_num/chi2.pdf}
%    \caption{}
%    \label{large condi num chi2 0.001}
%\end{subfigure}%
%\begin{subfigure}{0.33\textwidth}
%    \centering
%    \includegraphics[width=\linewidth]
%        {./images/0.001/large_condition_num/eta.pdf}
%    \caption{}
%    \label{large condi num eta 0.001}
%\end{subfigure}
%\caption{In this case all frequencies are the same as
%    Figure(\ref{large condi num}) except $f_{\text{scan}} = 0.001$.
%}
%\label{large condi num 0.001}
%\end{figure}
%
%\begin{figure}[htb]
%\centering
%\begin{subfigure}{0.33\textwidth}
%    \centering
%    \includegraphics[width=\linewidth]
%        {./images/10/large_condition_num/p_f.pdf}
%    \caption{}
%    \label{large condi num power spectrum 10}
%\end{subfigure}%
%\begin{subfigure}{0.33\textwidth}
%    \centering
%    \includegraphics[width=\linewidth]
%        {./images/10/large_condition_num/chi2.pdf}
%    \caption{}
%    \label{large condi num chi2 10}
%\end{subfigure}%
%\begin{subfigure}{0.33\textwidth}
%    \centering
%    \includegraphics[width=\linewidth]
%        {./images/10/large_condition_num/eta.pdf}
%    \caption{}
%    \label{large condi num eta 10}
%\end{subfigure}
%\caption{In this case all frequencies are the same as
%    Figure(\ref{large condi num}) except $f_{\text{scan}} = 10$.
%}
%\label{large condi num 10}
%\end{figure}
%
%
%
%\section{Conclusion}
%Here we discussed a method to solve map making equation
%Eq.(\ref{map making equation})
%\begin{align}
%\hatm = \PPinv{N} \Pdagger \inv{N} \vbd \tag{\ref{map making equation}}
%\end{align}
%by separating noise covariance matrix $N$ into two parts, white noise part
%$\tau I$ and the remaining noise $\Nbar$.
%Then we could think $\Nbar$ as a perturbation added to white noise, 
%by introducing a parameter $\eta$, as $\eta$ change from $0$ to $1$,
%we gradually add this non white noise in to system.
%
%The $\eta$ values can be predetermined analytically.
%This property is very important, because we don't want to keep entire time
%ordered data in system RAM.
%If these $\eta$ values can be determined before calculation, then we only need
%to keep several map sized object, which is much smaller than timed ordered 
%data.
%Also we showed that this method has same computational cost as vanilla
%conjugate gradient method but performs better when the condition number of 
%noise covariance matrix $\kappa(N)$ is large, especially in $1/f$ noise case.
%The only extra free parameter added is to determine whether the error at
%current step $\vb{r}(\eta_i) = \qty||\vbb(\eta_i) - A(\eta_i) \vbm||$ is small
%enough such that we change advance to next value $\eta_{i+1}$.
%
%The perturbation parameter $\eta$ get from Eq.(\ref{etai rule}) are not
%perfect. Since it only takes in to account the noise information in $N$,
%but ignored all scanning information contained in pointing matrix $P$, because
%we are unable to analyze the structure of
%$\vb{r}_{\eta} = \vbd - P\hatm(\eta) = \mathcal{P}_{\eta} \vbd$ in frequency
%space.
%
%The analysis of $\eta$ value also explains why cooling parameters
%$\lambda=1/\eta$ in messenger field are chosen to be geometric series or
%\texttt{logspace} \cite{Huffenberger_2018}.
%
%All of the calculation are using simple preconditioner $\Pdagger P$, but 
%the entire analysis is independent of preconditioner.
%Using better preconditioners, it would also have improvements.





\medskip

\bibliographystyle{plain}
\bibliography{references.bib}




\end{document}


